{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DSC478_A3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpTIVS5RU3Oq",
        "colab_type": "text"
      },
      "source": [
        "#Assignment 3\n",
        "\n",
        "## DSC 478 - Programming ML Apps\n",
        "## Alex Teboul"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRfwmUCeVRYp",
        "colab_type": "text"
      },
      "source": [
        "##Assignment Background\n",
        "\n",
        "For this assignment you will experiment with various regression approaches and you'll get your feet wet with some clustering. We will rely on subsets of some real-world data sets and on tools from the scikit-learn machine learning package for Python as well as modules from the textbook code (Machine Learning in Action, Chapters 8 and 10)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEjo765CViFy",
        "colab_type": "text"
      },
      "source": [
        "# Problem 1 - Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ig6dcFK7Vodf",
        "colab_type": "text"
      },
      "source": [
        "## a.\n",
        "\n",
        "Load and preprocess the data using Pandas or Numpy and, if necessary, preprocessing functions from scikit-learn. The provided data is already normalized (see description), so there is no need for additional normalization. Compute and display basic statistics (mean, standard deviation, min, max, etc.) for each of the variables in the data set. Separate the target attribute for regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLN2uiqHZ8dS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#imports\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7zL8qYcsdFF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2bf4eaa5-97e6-470b-b4d6-e579feeb2122"
      },
      "source": [
        "#Mount Google Drive to get files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNKO1TYpsrdB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "47754747-9f51-4aae-e15e-5b07bafb8482"
      },
      "source": [
        "#Files\n",
        "#train_matrix\n",
        "compath = '/content/drive/My Drive/Colab Notebooks/datasets/communities/communities.csv'\n",
        "com_data = pd.read_csv(compath, sep=\",\",na_values='?')\n",
        "com_data.shape"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1994, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoM63pYGAoUo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "a8d13716-b299-45f4-f49b-37fe781ed220"
      },
      "source": [
        "#impute with median because of skew\n",
        "com_data.OtherPerCap.hist(bins=50)"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f7177127f28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 203
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo1l-l8qAA44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# deal with '?' value in OtherPerCap\n",
        "com_data.OtherPerCap.fillna(com_data.OtherPerCap.median(), inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adWDS_LIeIQz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "183e6038-d912-425b-9aed-ac4d7b642b95"
      },
      "source": [
        "com_data.head()"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>state</th>\n",
              "      <th>communityname</th>\n",
              "      <th>population</th>\n",
              "      <th>householdsize</th>\n",
              "      <th>racepctblack</th>\n",
              "      <th>racePctWhite</th>\n",
              "      <th>racePctAsian</th>\n",
              "      <th>racePctHisp</th>\n",
              "      <th>agePct12t21</th>\n",
              "      <th>agePct12t29</th>\n",
              "      <th>agePct16t24</th>\n",
              "      <th>agePct65up</th>\n",
              "      <th>numbUrban</th>\n",
              "      <th>pctUrban</th>\n",
              "      <th>medIncome</th>\n",
              "      <th>pctWWage</th>\n",
              "      <th>pctWFarmSelf</th>\n",
              "      <th>pctWInvInc</th>\n",
              "      <th>pctWSocSec</th>\n",
              "      <th>pctWPubAsst</th>\n",
              "      <th>pctWRetire</th>\n",
              "      <th>medFamInc</th>\n",
              "      <th>perCapInc</th>\n",
              "      <th>whitePerCap</th>\n",
              "      <th>blackPerCap</th>\n",
              "      <th>indianPerCap</th>\n",
              "      <th>AsianPerCap</th>\n",
              "      <th>OtherPerCap</th>\n",
              "      <th>HispPerCap</th>\n",
              "      <th>NumUnderPov</th>\n",
              "      <th>PctPopUnderPov</th>\n",
              "      <th>PctLess9thGrade</th>\n",
              "      <th>PctNotHSGrad</th>\n",
              "      <th>PctBSorMore</th>\n",
              "      <th>PctUnemployed</th>\n",
              "      <th>PctEmploy</th>\n",
              "      <th>PctEmplManu</th>\n",
              "      <th>PctEmplProfServ</th>\n",
              "      <th>MalePctDivorce</th>\n",
              "      <th>MalePctNevMarr</th>\n",
              "      <th>...</th>\n",
              "      <th>PctSpeakEnglOnly</th>\n",
              "      <th>PctNotSpeakEnglWell</th>\n",
              "      <th>PctLargHouseFam</th>\n",
              "      <th>PctLargHouseOccup</th>\n",
              "      <th>PersPerOccupHous</th>\n",
              "      <th>PersPerOwnOccHous</th>\n",
              "      <th>PersPerRentOccHous</th>\n",
              "      <th>PctPersOwnOccup</th>\n",
              "      <th>PctPersDenseHous</th>\n",
              "      <th>PctHousLess3BR</th>\n",
              "      <th>MedNumBR</th>\n",
              "      <th>HousVacant</th>\n",
              "      <th>PctHousOccup</th>\n",
              "      <th>PctHousOwnOcc</th>\n",
              "      <th>PctVacantBoarded</th>\n",
              "      <th>PctVacMore6Mos</th>\n",
              "      <th>MedYrHousBuilt</th>\n",
              "      <th>PctHousNoPhone</th>\n",
              "      <th>PctWOFullPlumb</th>\n",
              "      <th>OwnOccLowQuart</th>\n",
              "      <th>OwnOccMedVal</th>\n",
              "      <th>OwnOccHiQuart</th>\n",
              "      <th>RentLowQ</th>\n",
              "      <th>RentMedian</th>\n",
              "      <th>RentHighQ</th>\n",
              "      <th>MedRent</th>\n",
              "      <th>MedRentPctHousInc</th>\n",
              "      <th>MedOwnCostPctInc</th>\n",
              "      <th>MedOwnCostPctIncNoMtg</th>\n",
              "      <th>NumInShelters</th>\n",
              "      <th>NumStreet</th>\n",
              "      <th>PctForeignBorn</th>\n",
              "      <th>PctBornSameState</th>\n",
              "      <th>PctSameHouse85</th>\n",
              "      <th>PctSameCity85</th>\n",
              "      <th>PctSameState85</th>\n",
              "      <th>LandArea</th>\n",
              "      <th>PopDens</th>\n",
              "      <th>PctUsePubTrans</th>\n",
              "      <th>ViolentCrimesPerPop</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8</td>\n",
              "      <td>Lakewoodcity</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.20</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.40</td>\n",
              "      <td>...</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>53</td>\n",
              "      <td>Tukwilacity</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.02</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.15</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.63</td>\n",
              "      <td>...</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>24</td>\n",
              "      <td>Aberdeentown</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.41</td>\n",
              "      <td>...</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>34</td>\n",
              "      <td>Willingborotownship</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.77</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.06</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.45</td>\n",
              "      <td>...</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.77</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.97</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>42</td>\n",
              "      <td>Bethlehemtownship</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.27</td>\n",
              "      <td>...</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.03</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   state        communityname  ...  PctUsePubTrans  ViolentCrimesPerPop\n",
              "0      8         Lakewoodcity  ...            0.20                 0.20\n",
              "1     53          Tukwilacity  ...            0.45                 0.67\n",
              "2     24         Aberdeentown  ...            0.02                 0.43\n",
              "3     34  Willingborotownship  ...            0.28                 0.12\n",
              "4     42    Bethlehemtownship  ...            0.02                 0.03\n",
              "\n",
              "[5 rows x 100 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74TOo0ywgzv8",
        "colab_type": "text"
      },
      "source": [
        "**Compute and display basic statistics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNQR68UvftAh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "25f87d55-fca1-4742-d01b-1e293aa962ae"
      },
      "source": [
        "com_data.describe()"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>state</th>\n",
              "      <th>population</th>\n",
              "      <th>householdsize</th>\n",
              "      <th>racepctblack</th>\n",
              "      <th>racePctWhite</th>\n",
              "      <th>racePctAsian</th>\n",
              "      <th>racePctHisp</th>\n",
              "      <th>agePct12t21</th>\n",
              "      <th>agePct12t29</th>\n",
              "      <th>agePct16t24</th>\n",
              "      <th>agePct65up</th>\n",
              "      <th>numbUrban</th>\n",
              "      <th>pctUrban</th>\n",
              "      <th>medIncome</th>\n",
              "      <th>pctWWage</th>\n",
              "      <th>pctWFarmSelf</th>\n",
              "      <th>pctWInvInc</th>\n",
              "      <th>pctWSocSec</th>\n",
              "      <th>pctWPubAsst</th>\n",
              "      <th>pctWRetire</th>\n",
              "      <th>medFamInc</th>\n",
              "      <th>perCapInc</th>\n",
              "      <th>whitePerCap</th>\n",
              "      <th>blackPerCap</th>\n",
              "      <th>indianPerCap</th>\n",
              "      <th>AsianPerCap</th>\n",
              "      <th>OtherPerCap</th>\n",
              "      <th>HispPerCap</th>\n",
              "      <th>NumUnderPov</th>\n",
              "      <th>PctPopUnderPov</th>\n",
              "      <th>PctLess9thGrade</th>\n",
              "      <th>PctNotHSGrad</th>\n",
              "      <th>PctBSorMore</th>\n",
              "      <th>PctUnemployed</th>\n",
              "      <th>PctEmploy</th>\n",
              "      <th>PctEmplManu</th>\n",
              "      <th>PctEmplProfServ</th>\n",
              "      <th>MalePctDivorce</th>\n",
              "      <th>MalePctNevMarr</th>\n",
              "      <th>FemalePctDiv</th>\n",
              "      <th>...</th>\n",
              "      <th>PctSpeakEnglOnly</th>\n",
              "      <th>PctNotSpeakEnglWell</th>\n",
              "      <th>PctLargHouseFam</th>\n",
              "      <th>PctLargHouseOccup</th>\n",
              "      <th>PersPerOccupHous</th>\n",
              "      <th>PersPerOwnOccHous</th>\n",
              "      <th>PersPerRentOccHous</th>\n",
              "      <th>PctPersOwnOccup</th>\n",
              "      <th>PctPersDenseHous</th>\n",
              "      <th>PctHousLess3BR</th>\n",
              "      <th>MedNumBR</th>\n",
              "      <th>HousVacant</th>\n",
              "      <th>PctHousOccup</th>\n",
              "      <th>PctHousOwnOcc</th>\n",
              "      <th>PctVacantBoarded</th>\n",
              "      <th>PctVacMore6Mos</th>\n",
              "      <th>MedYrHousBuilt</th>\n",
              "      <th>PctHousNoPhone</th>\n",
              "      <th>PctWOFullPlumb</th>\n",
              "      <th>OwnOccLowQuart</th>\n",
              "      <th>OwnOccMedVal</th>\n",
              "      <th>OwnOccHiQuart</th>\n",
              "      <th>RentLowQ</th>\n",
              "      <th>RentMedian</th>\n",
              "      <th>RentHighQ</th>\n",
              "      <th>MedRent</th>\n",
              "      <th>MedRentPctHousInc</th>\n",
              "      <th>MedOwnCostPctInc</th>\n",
              "      <th>MedOwnCostPctIncNoMtg</th>\n",
              "      <th>NumInShelters</th>\n",
              "      <th>NumStreet</th>\n",
              "      <th>PctForeignBorn</th>\n",
              "      <th>PctBornSameState</th>\n",
              "      <th>PctSameHouse85</th>\n",
              "      <th>PctSameCity85</th>\n",
              "      <th>PctSameState85</th>\n",
              "      <th>LandArea</th>\n",
              "      <th>PopDens</th>\n",
              "      <th>PctUsePubTrans</th>\n",
              "      <th>ViolentCrimesPerPop</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1994.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>28.683551</td>\n",
              "      <td>0.057593</td>\n",
              "      <td>0.463395</td>\n",
              "      <td>0.179629</td>\n",
              "      <td>0.753716</td>\n",
              "      <td>0.153681</td>\n",
              "      <td>0.144022</td>\n",
              "      <td>0.424218</td>\n",
              "      <td>0.493867</td>\n",
              "      <td>0.336264</td>\n",
              "      <td>0.423164</td>\n",
              "      <td>0.064072</td>\n",
              "      <td>0.696269</td>\n",
              "      <td>0.361123</td>\n",
              "      <td>0.558154</td>\n",
              "      <td>0.291570</td>\n",
              "      <td>0.495687</td>\n",
              "      <td>0.471133</td>\n",
              "      <td>0.317778</td>\n",
              "      <td>0.479248</td>\n",
              "      <td>0.375677</td>\n",
              "      <td>0.350251</td>\n",
              "      <td>0.368049</td>\n",
              "      <td>0.291098</td>\n",
              "      <td>0.203506</td>\n",
              "      <td>0.322357</td>\n",
              "      <td>0.284724</td>\n",
              "      <td>0.386279</td>\n",
              "      <td>0.055507</td>\n",
              "      <td>0.303024</td>\n",
              "      <td>0.315807</td>\n",
              "      <td>0.383330</td>\n",
              "      <td>0.361675</td>\n",
              "      <td>0.363531</td>\n",
              "      <td>0.501073</td>\n",
              "      <td>0.396384</td>\n",
              "      <td>0.440597</td>\n",
              "      <td>0.461244</td>\n",
              "      <td>0.434453</td>\n",
              "      <td>0.487568</td>\n",
              "      <td>...</td>\n",
              "      <td>0.785903</td>\n",
              "      <td>0.150587</td>\n",
              "      <td>0.267608</td>\n",
              "      <td>0.251891</td>\n",
              "      <td>0.462101</td>\n",
              "      <td>0.494428</td>\n",
              "      <td>0.404097</td>\n",
              "      <td>0.562598</td>\n",
              "      <td>0.186264</td>\n",
              "      <td>0.495186</td>\n",
              "      <td>0.314694</td>\n",
              "      <td>0.076815</td>\n",
              "      <td>0.719549</td>\n",
              "      <td>0.548686</td>\n",
              "      <td>0.204529</td>\n",
              "      <td>0.433335</td>\n",
              "      <td>0.494178</td>\n",
              "      <td>0.264478</td>\n",
              "      <td>0.243059</td>\n",
              "      <td>0.264689</td>\n",
              "      <td>0.263490</td>\n",
              "      <td>0.268942</td>\n",
              "      <td>0.346379</td>\n",
              "      <td>0.372457</td>\n",
              "      <td>0.422964</td>\n",
              "      <td>0.384102</td>\n",
              "      <td>0.490125</td>\n",
              "      <td>0.449754</td>\n",
              "      <td>0.403816</td>\n",
              "      <td>0.029438</td>\n",
              "      <td>0.022778</td>\n",
              "      <td>0.215552</td>\n",
              "      <td>0.608892</td>\n",
              "      <td>0.535050</td>\n",
              "      <td>0.626424</td>\n",
              "      <td>0.651530</td>\n",
              "      <td>0.065231</td>\n",
              "      <td>0.232854</td>\n",
              "      <td>0.161685</td>\n",
              "      <td>0.237979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>16.397553</td>\n",
              "      <td>0.126906</td>\n",
              "      <td>0.163717</td>\n",
              "      <td>0.253442</td>\n",
              "      <td>0.244039</td>\n",
              "      <td>0.208877</td>\n",
              "      <td>0.232492</td>\n",
              "      <td>0.155196</td>\n",
              "      <td>0.143564</td>\n",
              "      <td>0.166505</td>\n",
              "      <td>0.179185</td>\n",
              "      <td>0.128256</td>\n",
              "      <td>0.444811</td>\n",
              "      <td>0.209362</td>\n",
              "      <td>0.182913</td>\n",
              "      <td>0.204108</td>\n",
              "      <td>0.178071</td>\n",
              "      <td>0.173619</td>\n",
              "      <td>0.222137</td>\n",
              "      <td>0.167564</td>\n",
              "      <td>0.198257</td>\n",
              "      <td>0.191109</td>\n",
              "      <td>0.186804</td>\n",
              "      <td>0.171593</td>\n",
              "      <td>0.164775</td>\n",
              "      <td>0.195411</td>\n",
              "      <td>0.190962</td>\n",
              "      <td>0.183081</td>\n",
              "      <td>0.127941</td>\n",
              "      <td>0.228474</td>\n",
              "      <td>0.213360</td>\n",
              "      <td>0.202508</td>\n",
              "      <td>0.209193</td>\n",
              "      <td>0.202171</td>\n",
              "      <td>0.174036</td>\n",
              "      <td>0.202386</td>\n",
              "      <td>0.175457</td>\n",
              "      <td>0.182460</td>\n",
              "      <td>0.175437</td>\n",
              "      <td>0.175170</td>\n",
              "      <td>...</td>\n",
              "      <td>0.226869</td>\n",
              "      <td>0.219716</td>\n",
              "      <td>0.196567</td>\n",
              "      <td>0.190709</td>\n",
              "      <td>0.169551</td>\n",
              "      <td>0.157924</td>\n",
              "      <td>0.189301</td>\n",
              "      <td>0.197087</td>\n",
              "      <td>0.209956</td>\n",
              "      <td>0.172508</td>\n",
              "      <td>0.255182</td>\n",
              "      <td>0.150465</td>\n",
              "      <td>0.194024</td>\n",
              "      <td>0.185204</td>\n",
              "      <td>0.217770</td>\n",
              "      <td>0.188986</td>\n",
              "      <td>0.232467</td>\n",
              "      <td>0.242847</td>\n",
              "      <td>0.206295</td>\n",
              "      <td>0.224425</td>\n",
              "      <td>0.231542</td>\n",
              "      <td>0.235252</td>\n",
              "      <td>0.219323</td>\n",
              "      <td>0.209278</td>\n",
              "      <td>0.248286</td>\n",
              "      <td>0.213404</td>\n",
              "      <td>0.169500</td>\n",
              "      <td>0.187274</td>\n",
              "      <td>0.192593</td>\n",
              "      <td>0.102607</td>\n",
              "      <td>0.100400</td>\n",
              "      <td>0.231134</td>\n",
              "      <td>0.204329</td>\n",
              "      <td>0.181352</td>\n",
              "      <td>0.200521</td>\n",
              "      <td>0.198221</td>\n",
              "      <td>0.109459</td>\n",
              "      <td>0.203092</td>\n",
              "      <td>0.229055</td>\n",
              "      <td>0.232985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>12.000000</td>\n",
              "      <td>0.010000</td>\n",
              "      <td>0.350000</td>\n",
              "      <td>0.020000</td>\n",
              "      <td>0.630000</td>\n",
              "      <td>0.040000</td>\n",
              "      <td>0.010000</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.410000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.440000</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.370000</td>\n",
              "      <td>0.350000</td>\n",
              "      <td>0.142500</td>\n",
              "      <td>0.360000</td>\n",
              "      <td>0.230000</td>\n",
              "      <td>0.220000</td>\n",
              "      <td>0.240000</td>\n",
              "      <td>0.172500</td>\n",
              "      <td>0.110000</td>\n",
              "      <td>0.190000</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.260000</td>\n",
              "      <td>0.010000</td>\n",
              "      <td>0.110000</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.230000</td>\n",
              "      <td>0.210000</td>\n",
              "      <td>0.220000</td>\n",
              "      <td>0.380000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>0.330000</td>\n",
              "      <td>0.310000</td>\n",
              "      <td>0.360000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.730000</td>\n",
              "      <td>0.030000</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.390000</td>\n",
              "      <td>0.270000</td>\n",
              "      <td>0.440000</td>\n",
              "      <td>0.060000</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010000</td>\n",
              "      <td>0.630000</td>\n",
              "      <td>0.430000</td>\n",
              "      <td>0.060000</td>\n",
              "      <td>0.290000</td>\n",
              "      <td>0.350000</td>\n",
              "      <td>0.060000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.090000</td>\n",
              "      <td>0.090000</td>\n",
              "      <td>0.090000</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.220000</td>\n",
              "      <td>0.210000</td>\n",
              "      <td>0.370000</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.060000</td>\n",
              "      <td>0.470000</td>\n",
              "      <td>0.420000</td>\n",
              "      <td>0.520000</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.020000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.020000</td>\n",
              "      <td>0.070000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>34.000000</td>\n",
              "      <td>0.020000</td>\n",
              "      <td>0.440000</td>\n",
              "      <td>0.060000</td>\n",
              "      <td>0.850000</td>\n",
              "      <td>0.070000</td>\n",
              "      <td>0.040000</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.480000</td>\n",
              "      <td>0.290000</td>\n",
              "      <td>0.420000</td>\n",
              "      <td>0.030000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.230000</td>\n",
              "      <td>0.480000</td>\n",
              "      <td>0.475000</td>\n",
              "      <td>0.260000</td>\n",
              "      <td>0.470000</td>\n",
              "      <td>0.330000</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.280000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.345000</td>\n",
              "      <td>0.020000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.270000</td>\n",
              "      <td>0.360000</td>\n",
              "      <td>0.310000</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>0.510000</td>\n",
              "      <td>0.370000</td>\n",
              "      <td>0.410000</td>\n",
              "      <td>0.470000</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.870000</td>\n",
              "      <td>0.060000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.190000</td>\n",
              "      <td>0.440000</td>\n",
              "      <td>0.480000</td>\n",
              "      <td>0.360000</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.110000</td>\n",
              "      <td>0.510000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.030000</td>\n",
              "      <td>0.770000</td>\n",
              "      <td>0.540000</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>0.420000</td>\n",
              "      <td>0.520000</td>\n",
              "      <td>0.185000</td>\n",
              "      <td>0.190000</td>\n",
              "      <td>0.180000</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.180000</td>\n",
              "      <td>0.310000</td>\n",
              "      <td>0.330000</td>\n",
              "      <td>0.370000</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.480000</td>\n",
              "      <td>0.450000</td>\n",
              "      <td>0.370000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>0.630000</td>\n",
              "      <td>0.540000</td>\n",
              "      <td>0.670000</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.040000</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.070000</td>\n",
              "      <td>0.150000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>42.000000</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.540000</td>\n",
              "      <td>0.230000</td>\n",
              "      <td>0.940000</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.470000</td>\n",
              "      <td>0.540000</td>\n",
              "      <td>0.360000</td>\n",
              "      <td>0.530000</td>\n",
              "      <td>0.070000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.490000</td>\n",
              "      <td>0.690000</td>\n",
              "      <td>0.370000</td>\n",
              "      <td>0.620000</td>\n",
              "      <td>0.580000</td>\n",
              "      <td>0.440000</td>\n",
              "      <td>0.580000</td>\n",
              "      <td>0.480000</td>\n",
              "      <td>0.430000</td>\n",
              "      <td>0.440000</td>\n",
              "      <td>0.380000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.360000</td>\n",
              "      <td>0.480000</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.450000</td>\n",
              "      <td>0.420000</td>\n",
              "      <td>0.510000</td>\n",
              "      <td>0.460000</td>\n",
              "      <td>0.480000</td>\n",
              "      <td>0.627500</td>\n",
              "      <td>0.520000</td>\n",
              "      <td>0.530000</td>\n",
              "      <td>0.590000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.620000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.940000</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.310000</td>\n",
              "      <td>0.290000</td>\n",
              "      <td>0.550000</td>\n",
              "      <td>0.580000</td>\n",
              "      <td>0.490000</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.220000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.070000</td>\n",
              "      <td>0.860000</td>\n",
              "      <td>0.670000</td>\n",
              "      <td>0.270000</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.670000</td>\n",
              "      <td>0.420000</td>\n",
              "      <td>0.330000</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.390000</td>\n",
              "      <td>0.380000</td>\n",
              "      <td>0.490000</td>\n",
              "      <td>0.520000</td>\n",
              "      <td>0.590000</td>\n",
              "      <td>0.530000</td>\n",
              "      <td>0.590000</td>\n",
              "      <td>0.580000</td>\n",
              "      <td>0.510000</td>\n",
              "      <td>0.010000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.280000</td>\n",
              "      <td>0.777500</td>\n",
              "      <td>0.660000</td>\n",
              "      <td>0.770000</td>\n",
              "      <td>0.790000</td>\n",
              "      <td>0.070000</td>\n",
              "      <td>0.280000</td>\n",
              "      <td>0.190000</td>\n",
              "      <td>0.330000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>56.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows Ã— 99 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             state   population  ...  PctUsePubTrans  ViolentCrimesPerPop\n",
              "count  1994.000000  1994.000000  ...     1994.000000          1994.000000\n",
              "mean     28.683551     0.057593  ...        0.161685             0.237979\n",
              "std      16.397553     0.126906  ...        0.229055             0.232985\n",
              "min       1.000000     0.000000  ...        0.000000             0.000000\n",
              "25%      12.000000     0.010000  ...        0.020000             0.070000\n",
              "50%      34.000000     0.020000  ...        0.070000             0.150000\n",
              "75%      42.000000     0.050000  ...        0.190000             0.330000\n",
              "max      56.000000     1.000000  ...        1.000000             1.000000\n",
              "\n",
              "[8 rows x 99 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8OmD9zng-Do",
        "colab_type": "text"
      },
      "source": [
        "**Separate the target attribute for regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkgQkc5iywru",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "9507abe9-71d3-48e8-fd44-15483e3dc2dc"
      },
      "source": [
        "com_x = com_data.drop(columns = ['state','communityname','ViolentCrimesPerPop'])\n",
        "com_x = com_x.astype('float64')\n",
        "com_x.head()"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>population</th>\n",
              "      <th>householdsize</th>\n",
              "      <th>racepctblack</th>\n",
              "      <th>racePctWhite</th>\n",
              "      <th>racePctAsian</th>\n",
              "      <th>racePctHisp</th>\n",
              "      <th>agePct12t21</th>\n",
              "      <th>agePct12t29</th>\n",
              "      <th>agePct16t24</th>\n",
              "      <th>agePct65up</th>\n",
              "      <th>numbUrban</th>\n",
              "      <th>pctUrban</th>\n",
              "      <th>medIncome</th>\n",
              "      <th>pctWWage</th>\n",
              "      <th>pctWFarmSelf</th>\n",
              "      <th>pctWInvInc</th>\n",
              "      <th>pctWSocSec</th>\n",
              "      <th>pctWPubAsst</th>\n",
              "      <th>pctWRetire</th>\n",
              "      <th>medFamInc</th>\n",
              "      <th>perCapInc</th>\n",
              "      <th>whitePerCap</th>\n",
              "      <th>blackPerCap</th>\n",
              "      <th>indianPerCap</th>\n",
              "      <th>AsianPerCap</th>\n",
              "      <th>OtherPerCap</th>\n",
              "      <th>HispPerCap</th>\n",
              "      <th>NumUnderPov</th>\n",
              "      <th>PctPopUnderPov</th>\n",
              "      <th>PctLess9thGrade</th>\n",
              "      <th>PctNotHSGrad</th>\n",
              "      <th>PctBSorMore</th>\n",
              "      <th>PctUnemployed</th>\n",
              "      <th>PctEmploy</th>\n",
              "      <th>PctEmplManu</th>\n",
              "      <th>PctEmplProfServ</th>\n",
              "      <th>MalePctDivorce</th>\n",
              "      <th>MalePctNevMarr</th>\n",
              "      <th>FemalePctDiv</th>\n",
              "      <th>TotalPctDiv</th>\n",
              "      <th>...</th>\n",
              "      <th>PctRecImmig10</th>\n",
              "      <th>PctSpeakEnglOnly</th>\n",
              "      <th>PctNotSpeakEnglWell</th>\n",
              "      <th>PctLargHouseFam</th>\n",
              "      <th>PctLargHouseOccup</th>\n",
              "      <th>PersPerOccupHous</th>\n",
              "      <th>PersPerOwnOccHous</th>\n",
              "      <th>PersPerRentOccHous</th>\n",
              "      <th>PctPersOwnOccup</th>\n",
              "      <th>PctPersDenseHous</th>\n",
              "      <th>PctHousLess3BR</th>\n",
              "      <th>MedNumBR</th>\n",
              "      <th>HousVacant</th>\n",
              "      <th>PctHousOccup</th>\n",
              "      <th>PctHousOwnOcc</th>\n",
              "      <th>PctVacantBoarded</th>\n",
              "      <th>PctVacMore6Mos</th>\n",
              "      <th>MedYrHousBuilt</th>\n",
              "      <th>PctHousNoPhone</th>\n",
              "      <th>PctWOFullPlumb</th>\n",
              "      <th>OwnOccLowQuart</th>\n",
              "      <th>OwnOccMedVal</th>\n",
              "      <th>OwnOccHiQuart</th>\n",
              "      <th>RentLowQ</th>\n",
              "      <th>RentMedian</th>\n",
              "      <th>RentHighQ</th>\n",
              "      <th>MedRent</th>\n",
              "      <th>MedRentPctHousInc</th>\n",
              "      <th>MedOwnCostPctInc</th>\n",
              "      <th>MedOwnCostPctIncNoMtg</th>\n",
              "      <th>NumInShelters</th>\n",
              "      <th>NumStreet</th>\n",
              "      <th>PctForeignBorn</th>\n",
              "      <th>PctBornSameState</th>\n",
              "      <th>PctSameHouse85</th>\n",
              "      <th>PctSameCity85</th>\n",
              "      <th>PctSameState85</th>\n",
              "      <th>LandArea</th>\n",
              "      <th>PopDens</th>\n",
              "      <th>PctUsePubTrans</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.19</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.20</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>...</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.02</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.15</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.91</td>\n",
              "      <td>1.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.70</td>\n",
              "      <td>...</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.04</td>\n",
              "      <td>0.77</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.06</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.44</td>\n",
              "      <td>...</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.77</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.97</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.01</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.21</td>\n",
              "      <td>...</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 97 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   population  householdsize  racepctblack  ...  LandArea  PopDens  PctUsePubTrans\n",
              "0        0.19           0.33          0.02  ...      0.12     0.26            0.20\n",
              "1        0.00           0.16          0.12  ...      0.02     0.12            0.45\n",
              "2        0.00           0.42          0.49  ...      0.01     0.21            0.02\n",
              "3        0.04           0.77          1.00  ...      0.02     0.39            0.28\n",
              "4        0.01           0.55          0.02  ...      0.04     0.09            0.02\n",
              "\n",
              "[5 rows x 97 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7Hn8TgQg-bx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "3b90d1ab-ab1c-4cea-a394-5d39c65b76cd"
      },
      "source": [
        "com_target = com_data[['ViolentCrimesPerPop']]\n",
        "com_target.head()"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ViolentCrimesPerPop</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.03</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ViolentCrimesPerPop\n",
              "0                 0.20\n",
              "1                 0.67\n",
              "2                 0.43\n",
              "3                 0.12\n",
              "4                 0.03"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LJ6aOuM3Y9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "com_x_npp = com_x.to_numpy()\n",
        "com_x_np = np.array([np.concatenate((v,[1.0])) for v in com_x_npp])\n",
        "com_target_series = pd.Series(com_target['ViolentCrimesPerPop'])\n",
        "com_target_np = com_target_series.to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeDuOn2q7was",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "cd8004f9-fbf0-4bff-fdb4-4b73b971e728"
      },
      "source": [
        "#Check x numpy array\n",
        "print(com_x_np[:2])"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.19 0.33 0.02 0.9  0.12 0.17 0.34 0.47 0.29 0.32 0.2  1.   0.37 0.72\n",
            "  0.34 0.6  0.29 0.15 0.43 0.39 0.4  0.39 0.32 0.27 0.27 0.36 0.41 0.08\n",
            "  0.19 0.1  0.18 0.48 0.27 0.68 0.23 0.41 0.68 0.4  0.75 0.75 0.35 0.55\n",
            "  0.59 0.61 0.56 0.74 0.76 0.04 0.14 0.03 0.24 0.27 0.37 0.39 0.07 0.07\n",
            "  0.08 0.08 0.89 0.06 0.14 0.13 0.33 0.39 0.28 0.55 0.09 0.51 0.5  0.21\n",
            "  0.71 0.52 0.05 0.26 0.65 0.14 0.06 0.22 0.19 0.18 0.36 0.35 0.38 0.34\n",
            "  0.38 0.46 0.25 0.04 0.   0.12 0.42 0.5  0.51 0.64 0.12 0.26 0.2  1.  ]\n",
            " [0.   0.16 0.12 0.74 0.45 0.07 0.26 0.59 0.35 0.27 0.02 1.   0.31 0.72\n",
            "  0.11 0.45 0.25 0.29 0.39 0.29 0.37 0.38 0.33 0.16 0.3  0.22 0.35 0.01\n",
            "  0.24 0.14 0.24 0.3  0.27 0.73 0.57 0.15 1.   0.63 0.91 1.   0.29 0.43\n",
            "  0.47 0.6  0.39 0.46 0.53 0.   0.24 0.01 0.52 0.62 0.64 0.63 0.25 0.27\n",
            "  0.25 0.23 0.84 0.1  0.16 0.1  0.17 0.29 0.17 0.26 0.2  0.82 0.   0.02\n",
            "  0.79 0.24 0.02 0.25 0.65 0.16 0.   0.21 0.2  0.21 0.42 0.38 0.4  0.37\n",
            "  0.29 0.32 0.18 0.   0.   0.21 0.5  0.34 0.6  0.52 0.02 0.12 0.45 1.  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhAoce197wd9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b1f28c86-44b3-4e39-f21a-65b024fa9321"
      },
      "source": [
        "#Check target numpy array\n",
        "print(com_target_np[:2])"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.2  0.67]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IwLfgeXVuQC",
        "colab_type": "text"
      },
      "source": [
        "## b.\n",
        "Perform standard linear regression on data using the implementation for Ch. 8 of MLA. Compute the RMSE value on the full training data. Also, plot the correlation between the predicted and actual values of the target attribute. Display the obtained regression coefficients (weights). Finally, perform 10-fold cross-validation and compare the cross-validation RMSE to the training RMSE (for cross validation, you should use the KFold module from sklearn.cross_validation)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awm7_GGoZ7g_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#MLA Ch.8 Standard Linear Regression\n",
        "def standRegres(xArr,yArr):\n",
        "    xMat = np.mat(xArr); yMat = np.mat(yArr).T\n",
        "    xTx = xMat.T*xMat\n",
        "    if np.linalg.det(xTx) == 0.0:\n",
        "        print (\"This matrix is singular, cannot do inverse\")\n",
        "        return\n",
        "    ws = xTx.I * (xMat.T*yMat)\n",
        "    return ws"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxKy_Y2nyDzj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c1cc7299-6cb5-4443-f4dd-a5380d60bd8e"
      },
      "source": [
        "#coefficients\n",
        "w = standRegres(com_x_np,com_target_np)\n",
        "w.shape"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(98, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZPNRxcR7hGg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "ae46032d-e65b-4c2f-c3b4-93d35de4e43b"
      },
      "source": [
        "#check first 10 coefficients\n",
        "print(w[:10])"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.13100493]\n",
            " [-0.03140101]\n",
            " [ 0.20990279]\n",
            " [-0.04056011]\n",
            " [-0.01389758]\n",
            " [ 0.05900913]\n",
            " [ 0.12333734]\n",
            " [-0.22263745]\n",
            " [-0.14742828]\n",
            " [ 0.05020185]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-CMU3k0WFMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#make predictions\n",
        "xMat=np.mat(com_x_np)\n",
        "yMat=np.mat(com_target_np)\n",
        "yHat = xMat*w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fBaCR50WRxT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4105bd74-2789-4454-d4b5-5dcce817cfb5"
      },
      "source": [
        "yHat.shape"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1994, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QvDbHeDWUwo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "c95b1cd9-4403-4ed6-c6d9-de2d31747f2c"
      },
      "source": [
        "print (yHat[0:10])"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.16553985]\n",
            " [ 0.29060553]\n",
            " [ 0.38367604]\n",
            " [ 0.32269589]\n",
            " [-0.02124193]\n",
            " [ 0.23017346]\n",
            " [ 0.08799567]\n",
            " [ 0.44236695]\n",
            " [ 0.23750267]\n",
            " [ 0.00519446]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTbgE0e-WcnM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "780ff821-ea56-42b1-bc0f-8122cd041c82"
      },
      "source": [
        "print(yMat.T[0:10])"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.2 ]\n",
            " [0.67]\n",
            " [0.43]\n",
            " [0.12]\n",
            " [0.03]\n",
            " [0.14]\n",
            " [0.03]\n",
            " [0.55]\n",
            " [0.53]\n",
            " [0.15]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG9NYAFBWg2T",
        "colab_type": "text"
      },
      "source": [
        "**Compute the RMSE on the full training data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbBZrDBZXEnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#transform predictions\n",
        "p = yHat.A.ravel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SsAepCFaYgr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "65001ba7-62de-4501-d66d-346840de7964"
      },
      "source": [
        "# Now we can constuct a vector of errors\n",
        "err = abs(p-com_target_np)\n",
        "\n",
        "# Let's see the error on the first 10 predictions\n",
        "print (err[:10])"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.03446015 0.37939447 0.04632396 0.20269589 0.05124193 0.09017346\n",
            " 0.05799567 0.10763305 0.29249733 0.14480554]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeIGItp1bqK9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c89fff6f-15ed-4750-e8a4-f33ed392b3e6"
      },
      "source": [
        "# Dot product of error vector with itself gives us the sum of squared errors\n",
        "total_error = np.dot(err,err)\n",
        "\n",
        "# Compute RMSE\n",
        "rmse_train = np.sqrt(total_error/len(p))\n",
        "print(\"RMSE Training Data:\", rmse_train)"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE Training Data: 0.1288876409101824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7i6tt6oabUu",
        "colab_type": "text"
      },
      "source": [
        "**Plot the correlation between the predicted and actual values of the target attribute**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-dRvSunaanD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "outputId": "cb682ede-312e-49b0-b4e9-2e9af9c86ab1"
      },
      "source": [
        "import pylab as pl\n",
        "# Plot outputs\n",
        "%matplotlib inline\n",
        "pl.plot(p, com_target_np,'bo')\n",
        "pl.plot([0,1],[0,1], 'r-')\n",
        "pl.xlabel('predicted')\n",
        "pl.ylabel('real')\n",
        "pl.title('real v predicted\\nViolentCrimesPerPop')\n",
        "pl.show()"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAElCAYAAADp4+XfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRWAYhtHimCIwDMNocUwRGIZhtDj/H+tGcqkZgW5zAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjyqhoxpd9bB",
        "colab_type": "text"
      },
      "source": [
        "**Display the obtained regression coefficients (weights)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJDjx9Q_h7sw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a5da983e-c8c9-4ef7-be9e-dd5c49dce396"
      },
      "source": [
        "#get shape\n",
        "w.shape"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(98, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_q3Dckce50Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "1aa6dcb3-311b-4a80-a1ce-17c174196b1d"
      },
      "source": [
        "#get values\n",
        "w.A.ravel()"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.31004930e-01, -3.14010065e-02,  2.09902786e-01, -4.05601115e-02,\n",
              "       -1.38975776e-02,  5.90091256e-02,  1.23337344e-01, -2.22637450e-01,\n",
              "       -1.47428281e-01,  5.02018454e-02, -2.42273093e-01,  4.63966980e-02,\n",
              "       -1.96960667e-01, -2.06125826e-01,  4.66013287e-02, -1.77224854e-01,\n",
              "        6.29780185e-02,  1.14964338e-02, -9.08903208e-02,  2.74640215e-01,\n",
              "        1.01812034e-01, -3.31565088e-01, -2.91956055e-02, -3.54522229e-02,\n",
              "        2.26288706e-02,  4.33296337e-02,  3.43620753e-02,  1.28366418e-01,\n",
              "       -1.91259421e-01, -1.00746784e-01,  6.46523213e-02,  1.06078903e-01,\n",
              "        1.56973896e-05,  2.34981121e-01, -3.75728828e-02, -7.74626866e-03,\n",
              "        4.66851160e-01,  2.26307821e-01,  1.74659870e-01, -5.75302969e-01,\n",
              "       -1.41945460e-01,  5.68725337e-02, -3.51039800e-01, -3.49743239e-02,\n",
              "        4.55798134e-04,  5.57032911e-02, -1.82246659e-01, -1.54637442e-01,\n",
              "        1.26177907e-01, -1.44299838e-01,  2.38863390e-02,  3.33947883e-02,\n",
              "       -7.42543109e-02,  3.59501818e-02, -3.31087287e-02, -2.18340383e-01,\n",
              "        4.45802715e-01, -1.99863777e-01, -2.66698133e-02, -1.41426506e-01,\n",
              "        6.37459345e-02, -2.10060128e-01,  6.51225133e-01, -8.02613856e-02,\n",
              "       -2.53786984e-01, -6.66372372e-01,  2.00984205e-01,  1.03290392e-01,\n",
              "        2.88585231e-02,  1.68307125e-01, -4.00774121e-02,  5.53915115e-01,\n",
              "        4.70359354e-02, -7.64451091e-02, -2.89327709e-02,  1.40357962e-02,\n",
              "       -1.40480129e-02, -3.46844573e-01,  2.67740928e-01,  1.19792680e-02,\n",
              "       -2.37011913e-01, -2.60921730e-02, -6.83408252e-02,  3.74649011e-01,\n",
              "        4.17147044e-02, -4.45462339e-02, -8.34601541e-02,  1.30733042e-01,\n",
              "        1.83469862e-01,  1.25977655e-01,  4.63943503e-03, -2.24429952e-02,\n",
              "        2.88850472e-02,  1.30485754e-02,  2.76063451e-02, -1.24568828e-02,\n",
              "       -3.73195160e-02,  5.88064969e-01])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Tf1zN-Hh0Ii",
        "colab_type": "text"
      },
      "source": [
        "**Finally, perform 10-fold cross-validation and compare the cross-validation RMSE to the training RMSE (for cross validation, you should use the KFold module from sklearn.cross_validation)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3C2GvyXyiGDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#imports\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score #uses KFold by default\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKNWsMk3ilLe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a201b77c-fdea-431b-f08c-b5599dcf3b26"
      },
      "source": [
        "# Compute RMSE using 10-fold x-validation\n",
        "# Create linear regression object\n",
        "linreg = LinearRegression() #yields the same result as the Ch 8 MLA example\n",
        "n = 10\n",
        "#kf = KFold(len(x),n_splits=n)\n",
        "\n",
        "kf = KFold(n_splits=n)\n",
        "kf.get_n_splits(com_x_np)\n",
        "\n",
        "print(kf)  \n",
        "KFold(n_splits=n,random_state=None, shuffle=False)\n",
        " \n",
        "\n",
        "xval_err = 0\n",
        "#or train,test in kf:\n",
        "for train, test in kf.split(com_x_np):\n",
        "    linreg.fit(com_x_np[train],com_target_np[train])\n",
        "    pr = linreg.predict(com_x_np[test])\n",
        "    e = pr-com_target_np[test]\n",
        "    xval_err += np.sqrt(np.dot(e,e)/len(com_x_np[test]))\n",
        "       \n",
        "rmse_10cv = xval_err/n"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KFold(n_splits=10, random_state=None, shuffle=False)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZxiQDCGPhb3",
        "colab_type": "text"
      },
      "source": [
        "**Compare RMSE on Training and 10-fold CV**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qjWQVexJYA6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c42012e3-03bc-41d0-b66d-f354cd176de3"
      },
      "source": [
        "method_name = 'Simple Linear Regression'\n",
        "print('Method: %s' %method_name)\n",
        "print('RMSE on training: %.4f' %rmse_train)\n",
        "print('RMSE on 10-fold CV: %.4f' %rmse_10cv)"
      ],
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Method: Simple Linear Regression\n",
            "RMSE on training: 0.1289\n",
            "RMSE on 10-fold CV: 0.1359\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X18GPV83SxJz",
        "colab_type": "text"
      },
      "source": [
        "*   The RMSE for Training and 10-Fold CV are close (0.1289 to 0.1359 respectively).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbVq1dmlVzyg",
        "colab_type": "text"
      },
      "source": [
        "## c.\n",
        "\n",
        "Feature Selection:  use the scikit-learn regression model from sklearn.linear_model with a subset of features to perform linear regression. For feature selection, write a script or function that takes as input the training data, target variable; the model; and any other parameters you find necessary, and returns the optimal percentage of the most informative features to use. Your approach should use k-fold cross-validation on the training data (you can use k=5). You can use feature_selection.SelectPercentile to find the most informative variables. Show the list of most informative variables and their weights [Note: since this is regression not classification, you should use feature_selection.f_regression as scoring function rather than chi2). Next, plot the model's mean absolute error values  on cross-validation relative to the percentage of selected features (See scikit-learn's metrics.mean_absolute_error). In order to use cross_validation.cross_val_score with regression you'll need to pass to it scoring='mean_absolute_error' as a parameter. [Hint: for an example of a similar feature selection process please review the class example notebook. Also, review scikit-learn documentation for feature selection.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_lvOXcCuj99",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "*   Note that cross_validation.cross_val_score has been deprecated. We can't actually assign mean_absolute_error scoring as directed in the homework prompt. model_selection.cross_val_score does accept 'neg_mean_absolute_error' though. So scores are multiplied by -1 for the appropriate score. Accuracy is also reported.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlAI5WLEZ7Fs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#imports\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import feature_selection\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import mean_absolute_error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amiyS7sNajUu",
        "colab_type": "text"
      },
      "source": [
        "**Show the list of most informative variables and their weights**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kRRrjE1d-Lc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "e3e9a546-84cc-4d0a-c7f3-17006ea5b6b7"
      },
      "source": [
        "import warnings; warnings.simplefilter('ignore')\n",
        "lm = LinearRegression()\n",
        "\n",
        "percentiles = range(1, 100, 5)\n",
        "results = []\n",
        "for i in range(1, 100, 5):\n",
        "    fs = feature_selection.SelectPercentile(feature_selection.f_regression, percentile=i)\n",
        "    X_train_fs = fs.fit_transform(com_x_np, com_target_np)\n",
        "    scores = model_selection.cross_val_score(lm, X_train_fs, com_target_np, scoring = 'neg_mean_absolute_error', cv=5)\n",
        "    print (i,scores.mean()*-1)\n",
        "    results = np.append(results, scores.mean()*-1)\n",
        "\n",
        "optimal_percentile = np.where(results == results.max())[0]\n",
        "print(optimal_percentile[0])\n",
        "print (\"Optimal percentile of features:{0}\".format(percentiles[optimal_percentile[0]]), \"\\n\")\n",
        "optimal_num_features = int(percentiles[optimal_percentile[0]]*len(com_x.columns)/100)\n",
        "print (\"Optimal number of features:{0}\".format(optimal_num_features), \"\\n\")"
      ],
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 0.11407909224209818\n",
            "6 0.1011518954003291\n",
            "11 0.10108738571229887\n",
            "16 0.1010448096187686\n",
            "21 0.09886424513530563\n",
            "26 0.09657268911607834\n",
            "31 0.09652815800003076\n",
            "36 0.09569677985937057\n",
            "41 0.0958121897628663\n",
            "46 0.09641184981816638\n",
            "51 0.09672043610039369\n",
            "56 0.09691763718778297\n",
            "61 0.09767947698307727\n",
            "66 0.09777190769444884\n",
            "71 0.09809307676734698\n",
            "76 0.09775414558827022\n",
            "81 0.09765589825801516\n",
            "86 0.0971248710056017\n",
            "91 0.09751928795970309\n",
            "96 0.09688361132326875\n",
            "0\n",
            "Optimal percentile of features:1 \n",
            "\n",
            "Optimal number of features:0 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh9hQunB3Stn",
        "colab_type": "text"
      },
      "source": [
        "**Plot the model's mean absolute error values on cross-validation relative to the percentage of selected features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-Sk-rLX3ONS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "acf6cb4c-08f7-41b5-d1a1-c443beebcb53"
      },
      "source": [
        "# Plot percentile of features VS. cross-validation scores\n",
        "import pylab as pl\n",
        "pl.figure()\n",
        "pl.xlabel(\"Percentage of features selected\")\n",
        "pl.ylabel(\"Cross validation \\n mean_absolute_error (MAE)\")\n",
        "pl.plot(percentiles,results)"
      ],
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f716e1d6b38>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 230
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEGCAYAAAAwpAFeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0pIHKOecc2nJA5Rzzrm05AHKOedcWvr/F8P8VitNm34AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYb-E_-R4Uwr",
        "colab_type": "text"
      },
      "source": [
        "**Accuracy plot for reference**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9gOsjlo3AUK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "outputId": "d1f99e7b-2bd2-4493-9501-1498c86d2b3f"
      },
      "source": [
        "import warnings; warnings.simplefilter('ignore')\n",
        "lm = LinearRegression()\n",
        "\n",
        "percentiles = range(1, 100, 5)\n",
        "results = []\n",
        "for i in range(1, 100, 5):\n",
        "    fs = feature_selection.SelectPercentile(feature_selection.f_regression, percentile=i)\n",
        "    X_train_fs = fs.fit_transform(com_x_np, com_target_np)\n",
        "    scores = model_selection.cross_val_score(lm, X_train_fs, com_target_np, cv=5)\n",
        "    print (i,scores.mean())\n",
        "    results = np.append(results, scores.mean())\n",
        "\n",
        "optimal_percentile = np.where(results == results.max())[0]\n",
        "print(optimal_percentile[0])\n",
        "print (\"Optimal percentile of features:{0}\".format(percentiles[optimal_percentile[0]]), \"\\n\")\n",
        "optimal_num_features = int(percentiles[optimal_percentile[0]]*len(com_x.columns)/100)\n",
        "print (\"Optimal number of features:{0}\".format(optimal_num_features), \"\\n\")\n",
        "\n",
        "# Plot percentile of features VS. cross-validation scores\n",
        "import pylab as pl\n",
        "pl.figure()\n",
        "pl.xlabel(\"Percentage of features selected\")\n",
        "pl.ylabel(\"Cross validation accuracy\")\n",
        "pl.plot(percentiles,results)"
      ],
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 0.5426547452399858\n",
            "6 0.6017067575632016\n",
            "11 0.611466074222782\n",
            "16 0.6120936164773824\n",
            "21 0.6284547541930063\n",
            "26 0.6420064965365857\n",
            "31 0.6414911742377145\n",
            "36 0.6449835857807804\n",
            "41 0.6447814085783337\n",
            "46 0.6419594293767954\n",
            "51 0.6402524990712182\n",
            "56 0.6407035764163449\n",
            "61 0.6398412807628088\n",
            "66 0.6396524924327041\n",
            "71 0.6391989417272407\n",
            "76 0.6427780074356966\n",
            "81 0.643631342753214\n",
            "86 0.6456205747318501\n",
            "91 0.6446374486824492\n",
            "96 0.6476345221750891\n",
            "19\n",
            "Optimal percentile of features:96 \n",
            "\n",
            "Optimal number of features:93 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f716e8567b8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 231
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEGCAYAAACdJRn3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\neG/NSm0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVbIRyIM87vw",
        "colab_type": "text"
      },
      "source": [
        "**Feature names**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di-i9lvT89vI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "3b0e5f06-952b-4385-98d7-9538e5a212ae"
      },
      "source": [
        "#feats = np.r_[1,6,11]\n",
        "zzzzz = com_x.iloc[:, np.r_[1,6,11,16,21,26,31,36,41,46,51,56,61,66,71,76,81,86,91,96]]\n",
        "zzzzz.columns"
      ],
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['householdsize', 'agePct12t21', 'pctUrban', 'pctWSocSec', 'whitePerCap',\n",
              "       'HispPerCap', 'PctBSorMore', 'MalePctDivorce', 'PctFam2Par',\n",
              "       'PctWorkMom', 'PctImmigRec5', 'PctRecImmig8', 'PctLargHouseOccup',\n",
              "       'PctPersDenseHous', 'PctHousOwnOcc', 'PctWOFullPlumb', 'RentMedian',\n",
              "       'MedOwnCostPctIncNoMtg', 'PctSameHouse85', 'PctUsePubTrans'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 232
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za6lk8QnV2OL",
        "colab_type": "text"
      },
      "source": [
        "## d.\n",
        "\n",
        "Next, perform Ridge Regression and Lasso Regression using the modules from sklearn.linear_model. In each case, perform systematic model selection to identify the optimal alpha parameter. First, create a 20%-80% randomized split of the data. Set aside the test portion; the model selection process should be performed using the 80% training data partition. You should create a function that takes as input the data and target variable; the parameter to vary and a list of its values; the model to be trained; and any other relevant input needed to determine the optimal value for the specified parameter. The model selection process should perform k-fold cross validation (k should be a parameter, but you can select k=5 for this problem). You should also plot the error values on the training and cross-validation splits across the specified values of the alpha parameter. Finally, using the best alpha value, run the model on the set-aside test data. Discuss your observation and conclusions. [Hint: for an example of a similar model selection process please review the class example notebook.]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ763M80MfSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Imports\n",
        "from sklearn.linear_model import Lasso, Ridge\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D78HB9FOSgtA",
        "colab_type": "text"
      },
      "source": [
        "**Create an 80/20 train test split**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSW8goV6SqgQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Split the data into train/test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(com_x_np, com_target_np, test_size=0.2, random_state=7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5carW-qZ6rY",
        "colab_type": "text"
      },
      "source": [
        "**Ridge Regression & Lasso Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-iC9te3LrVP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "b6deba38-b8eb-4d7b-cccf-d7cc7f992474"
      },
      "source": [
        "#a = 0.2\n",
        "k=5\n",
        "\n",
        "X = x_train\n",
        "y = y_train\n",
        "\n",
        "for name,met in [\n",
        "        #('lasso', Lasso(fit_intercept=True, alpha=a)),\n",
        "        ('lasso', Lasso(fit_intercept=True)),\n",
        "        #('ridge', Ridge(fit_intercept=True, alpha=a)),\n",
        "        ('ridge', Ridge(fit_intercept=True)),\n",
        "        ]:\n",
        "    met.fit(X,y)\n",
        "    p = met.predict(X)\n",
        "    e = p-y\n",
        "    total_error = np.dot(e,e)\n",
        "    rmse_train = np.sqrt(total_error/len(p))\n",
        "\n",
        "    #kf = KFold(len(x), n_folds=5)\n",
        "    kf = KFold(n_splits=k)\n",
        "    kf.get_n_splits(X)\n",
        "\n",
        "    KFold(n_splits=n,random_state=None, shuffle=False)\n",
        "    err = 0\n",
        "    for train,test in kf.split(X):\n",
        "        met.fit(X[train],y[train])\n",
        "        p = met.predict(X[test])\n",
        "        e = p-y[test]\n",
        "        err += np.dot(e,e)\n",
        "    rmse_10cv = np.sqrt(err/len(X))\n",
        "    \n",
        "    print('Method: %s' %name)\n",
        "    print('RMSE on training: %.4f' %rmse_train)\n",
        "    print('RMSE on 10-fold CV: %.4f' %rmse_10cv)\n",
        "    print (\"\\n\")"
      ],
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Method: lasso\n",
            "RMSE on training: 0.2326\n",
            "RMSE on 10-fold CV: 0.2327\n",
            "\n",
            "\n",
            "Method: ridge\n",
            "RMSE on training: 0.1305\n",
            "RMSE on 10-fold CV: 0.1382\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfJ411USHOyu",
        "colab_type": "text"
      },
      "source": [
        "**Ridge Regression alpha parameter tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNfEDj6AMHMR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        },
        "outputId": "96075738-7f78-49ee-d808-e2448117a640"
      },
      "source": [
        "print('Ridge Regression')\n",
        "print('alpha\\t RMSE_train\\t RMSE_10cv\\n')\n",
        "alpha = np.linspace(.01,10,50)\n",
        "t_rmse = np.array([])\n",
        "cv_rmse = np.array([])\n",
        "k=5\n",
        "for a in alpha:\n",
        "    ridge = Ridge(alpha=a)\n",
        "    \n",
        "    # computing the RMSE on training data\n",
        "    ridge.fit(X,y)\n",
        "    p = ridge.predict(X)\n",
        "    err = p-y\n",
        "    total_error = np.dot(err,err)\n",
        "    rmse_train = np.sqrt(total_error/len(p))\n",
        "\n",
        "    kf = KFold(n_splits=k)\n",
        "    kf.get_n_splits(X)\n",
        " \n",
        "    KFold(n_splits=k,random_state=None, shuffle=False)\n",
        "    # computing RMSE using 5-fold cross validation\n",
        "    #kf = KFold(len(X), n_folds=5)\n",
        "    xval_err = 0\n",
        "    for train, test in kf.split(X):\n",
        "        ridge.fit(X[train], y[train])\n",
        "        p = ridge.predict(X[test])\n",
        "        err = p - y[test]\n",
        "        xval_err += np.sqrt(np.dot(err,err)/len(X[test]))\n",
        "    rmse_10cv = xval_err/n\n",
        "    \n",
        "    t_rmse = np.append(t_rmse, [rmse_train])\n",
        "    cv_rmse = np.append(cv_rmse, [rmse_10cv])\n",
        "    print('{:.3f}\\t {:.4f}\\t\\t {:.4f}'.format(a,rmse_train,rmse_10cv))"
      ],
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ridge Regression\n",
            "alpha\t RMSE_train\t RMSE_10cv\n",
            "\n",
            "0.010\t 0.1293\t\t 0.0693\n",
            "0.214\t 0.1296\t\t 0.0689\n",
            "0.418\t 0.1298\t\t 0.0688\n",
            "0.622\t 0.1301\t\t 0.0688\n",
            "0.826\t 0.1303\t\t 0.0688\n",
            "1.029\t 0.1305\t\t 0.0688\n",
            "1.233\t 0.1307\t\t 0.0688\n",
            "1.437\t 0.1308\t\t 0.0689\n",
            "1.641\t 0.1310\t\t 0.0689\n",
            "1.845\t 0.1312\t\t 0.0689\n",
            "2.049\t 0.1313\t\t 0.0689\n",
            "2.253\t 0.1315\t\t 0.0689\n",
            "2.457\t 0.1316\t\t 0.0690\n",
            "2.660\t 0.1317\t\t 0.0690\n",
            "2.864\t 0.1318\t\t 0.0690\n",
            "3.068\t 0.1319\t\t 0.0690\n",
            "3.272\t 0.1321\t\t 0.0690\n",
            "3.476\t 0.1322\t\t 0.0691\n",
            "3.680\t 0.1323\t\t 0.0691\n",
            "3.884\t 0.1324\t\t 0.0691\n",
            "4.088\t 0.1325\t\t 0.0691\n",
            "4.291\t 0.1325\t\t 0.0691\n",
            "4.495\t 0.1326\t\t 0.0692\n",
            "4.699\t 0.1327\t\t 0.0692\n",
            "4.903\t 0.1328\t\t 0.0692\n",
            "5.107\t 0.1329\t\t 0.0692\n",
            "5.311\t 0.1330\t\t 0.0692\n",
            "5.515\t 0.1330\t\t 0.0692\n",
            "5.719\t 0.1331\t\t 0.0692\n",
            "5.922\t 0.1332\t\t 0.0693\n",
            "6.126\t 0.1332\t\t 0.0693\n",
            "6.330\t 0.1333\t\t 0.0693\n",
            "6.534\t 0.1334\t\t 0.0693\n",
            "6.738\t 0.1334\t\t 0.0693\n",
            "6.942\t 0.1335\t\t 0.0693\n",
            "7.146\t 0.1336\t\t 0.0693\n",
            "7.350\t 0.1336\t\t 0.0693\n",
            "7.553\t 0.1337\t\t 0.0694\n",
            "7.757\t 0.1337\t\t 0.0694\n",
            "7.961\t 0.1338\t\t 0.0694\n",
            "8.165\t 0.1338\t\t 0.0694\n",
            "8.369\t 0.1339\t\t 0.0694\n",
            "8.573\t 0.1340\t\t 0.0694\n",
            "8.777\t 0.1340\t\t 0.0694\n",
            "8.981\t 0.1341\t\t 0.0694\n",
            "9.184\t 0.1341\t\t 0.0694\n",
            "9.388\t 0.1342\t\t 0.0694\n",
            "9.592\t 0.1342\t\t 0.0695\n",
            "9.796\t 0.1342\t\t 0.0695\n",
            "10.000\t 0.1343\t\t 0.0695\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBjPWxd4Hl8B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "751c7528-d365-4080-a4b0-f195a652e3e7"
      },
      "source": [
        "pl.plot(alpha, t_rmse, label='RMSE-Train')\n",
        "pl.plot(alpha, cv_rmse, label='RMSE_XVal')\n",
        "pl.legend( ('RMSE-Train', 'RMSE_XVal') )\n",
        "pl.ylabel('RMSE')\n",
        "pl.xlabel('Alpha')\n",
        "pl.title('Ridge Regession Alpha Parameter Tuning')\n",
        "pl.show()"
      ],
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_R3ySS_ICIR",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "*   alpha = 0.3 is a good enough choice for this Ridge model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeA1Ln6rIMzs",
        "colab_type": "text"
      },
      "source": [
        "**Lasso Regression alpha parameter tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82gcM2T3IN12",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        },
        "outputId": "38b67f04-e15f-4fea-8fa0-9c999eaa7dfd"
      },
      "source": [
        "print('Lasso Regression')\n",
        "print('alpha\\t RMSE_train\\t RMSE_10cv\\n')\n",
        "alpha = np.linspace(.001,.05,50)\n",
        "t_rmse = np.array([])\n",
        "cv_rmse = np.array([])\n",
        "k=5\n",
        "for a in alpha:\n",
        "    lasso = Lasso(alpha=a)\n",
        "    \n",
        "    # computing the RMSE on training data\n",
        "    lasso.fit(X,y)\n",
        "    p = lasso.predict(X)\n",
        "    err = p-y\n",
        "    total_error = np.dot(err,err)\n",
        "    rmse_train = np.sqrt(total_error/len(p))\n",
        "\n",
        "    kf = KFold(n_splits=k)\n",
        "    kf.get_n_splits(X)\n",
        " \n",
        "    KFold(n_splits=k,random_state=None, shuffle=False)\n",
        "    # computing RMSE using 5-fold cross validation\n",
        "    #kf = KFold(len(X), n_folds=5)\n",
        "    xval_err = 0\n",
        "    for train, test in kf.split(X):\n",
        "        lasso.fit(X[train], y[train])\n",
        "        p = lasso.predict(X[test])\n",
        "        err = p - y[test]\n",
        "        xval_err += np.sqrt(np.dot(err,err)/len(X[test]))\n",
        "    rmse_10cv = xval_err/n\n",
        "    \n",
        "    t_rmse = np.append(t_rmse, [rmse_train])\n",
        "    cv_rmse = np.append(cv_rmse, [rmse_10cv])\n",
        "    print('{:.3f}\\t {:.4f}\\t\\t {:.4f}'.format(a,rmse_train,rmse_10cv))"
      ],
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lasso Regression\n",
            "alpha\t RMSE_train\t RMSE_10cv\n",
            "\n",
            "0.001\t 0.1375\t\t 0.0697\n",
            "0.002\t 0.1398\t\t 0.0706\n",
            "0.003\t 0.1418\t\t 0.0713\n",
            "0.004\t 0.1437\t\t 0.0722\n",
            "0.005\t 0.1459\t\t 0.0733\n",
            "0.006\t 0.1486\t\t 0.0746\n",
            "0.007\t 0.1516\t\t 0.0759\n",
            "0.008\t 0.1537\t\t 0.0770\n",
            "0.009\t 0.1558\t\t 0.0780\n",
            "0.010\t 0.1577\t\t 0.0789\n",
            "0.011\t 0.1593\t\t 0.0797\n",
            "0.012\t 0.1611\t\t 0.0806\n",
            "0.013\t 0.1629\t\t 0.0815\n",
            "0.014\t 0.1650\t\t 0.0825\n",
            "0.015\t 0.1671\t\t 0.0836\n",
            "0.016\t 0.1694\t\t 0.0847\n",
            "0.017\t 0.1717\t\t 0.0859\n",
            "0.018\t 0.1742\t\t 0.0872\n",
            "0.019\t 0.1768\t\t 0.0885\n",
            "0.020\t 0.1795\t\t 0.0898\n",
            "0.021\t 0.1822\t\t 0.0912\n",
            "0.022\t 0.1851\t\t 0.0926\n",
            "0.023\t 0.1879\t\t 0.0939\n",
            "0.024\t 0.1904\t\t 0.0952\n",
            "0.025\t 0.1929\t\t 0.0965\n",
            "0.026\t 0.1956\t\t 0.0978\n",
            "0.027\t 0.1982\t\t 0.0992\n",
            "0.028\t 0.2010\t\t 0.1006\n",
            "0.029\t 0.2038\t\t 0.1020\n",
            "0.030\t 0.2067\t\t 0.1034\n",
            "0.031\t 0.2096\t\t 0.1048\n",
            "0.032\t 0.2126\t\t 0.1063\n",
            "0.033\t 0.2157\t\t 0.1078\n",
            "0.034\t 0.2186\t\t 0.1093\n",
            "0.035\t 0.2216\t\t 0.1108\n",
            "0.036\t 0.2246\t\t 0.1123\n",
            "0.037\t 0.2277\t\t 0.1138\n",
            "0.038\t 0.2308\t\t 0.1151\n",
            "0.039\t 0.2326\t\t 0.1157\n",
            "0.040\t 0.2326\t\t 0.1161\n",
            "0.041\t 0.2326\t\t 0.1161\n",
            "0.042\t 0.2326\t\t 0.1161\n",
            "0.043\t 0.2326\t\t 0.1161\n",
            "0.044\t 0.2326\t\t 0.1161\n",
            "0.045\t 0.2326\t\t 0.1161\n",
            "0.046\t 0.2326\t\t 0.1161\n",
            "0.047\t 0.2326\t\t 0.1161\n",
            "0.048\t 0.2326\t\t 0.1161\n",
            "0.049\t 0.2326\t\t 0.1161\n",
            "0.050\t 0.2326\t\t 0.1161\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmeaqXErJh_x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "4926a7cd-cca4-4846-80ad-76dd2c0c9884"
      },
      "source": [
        "pl.plot(alpha, t_rmse, label='RMSE-Train')\n",
        "pl.plot(alpha, cv_rmse, label='RMSE_XVal')\n",
        "pl.legend( ('RMSE-Train', 'RMSE_XVal') )\n",
        "pl.ylabel('RMSE')\n",
        "pl.xlabel('Alpha')\n",
        "pl.title('Lasso Regession Alpha Parameter Tuning')\n",
        "pl.show()"
      ],
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFDce71ULPYw",
        "colab_type": "text"
      },
      "source": [
        "*  alpha closer to zero the better, but around 0.04 maybe more stable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRzf7A0eNdeJ",
        "colab_type": "text"
      },
      "source": [
        "**Finally, using the best alpha value, run the model on the set-aside test data. Discuss your observation and conclusions. (Ridge, alpha = 0.3)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXrEqO2INEnf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a6ec18a0-a348-4f70-dd23-cb04337240b8"
      },
      "source": [
        "a = 0.3\n",
        "k=5\n",
        "\n",
        "X_t = x_test\n",
        "y_t = y_test\n",
        "\n",
        "for name,met in [\n",
        "        ('ridge', Ridge(fit_intercept=True, alpha=a)),\n",
        "        ]:\n",
        "    met.fit(X_t,y_t)\n",
        "    p = met.predict(X_t)\n",
        "    e = p-y_t\n",
        "    total_error = np.dot(e,e)\n",
        "    rmse_train = np.sqrt(total_error/len(p))\n",
        "\n",
        "    #kf = KFold(len(X_t), n_folds=5)\n",
        "    kf = KFold(n_splits=k)\n",
        "    kf.get_n_splits(X_t)\n",
        "\n",
        "    KFold(n_splits=n,random_state=None, shuffle=False)\n",
        "    err = 0\n",
        "    for train,test in kf.split(X_t):\n",
        "        met.fit(X_t[train],y_t[train])\n",
        "        p = met.predict(X_t[test])\n",
        "        e = p-y_t[test]\n",
        "        err += np.dot(e,e)\n",
        "    rmse_10cv = np.sqrt(err/len(X_t))\n",
        "    \n",
        "    print('Method with alpha = 0.3: %s' %name)\n",
        "    print('RMSE on testing: %.4f' %rmse_train)\n",
        "    print('RMSE on 5-fold CV: %.4f' %rmse_10cv)\n",
        "    print (\"\\n\")"
      ],
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Method with alpha = 0.3: ridge\n",
            "RMSE on testing: 0.1081\n",
            "RMSE on 5-fold CV: 0.1344\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFTyMPe1OIxi",
        "colab_type": "text"
      },
      "source": [
        "*  It appears that Ridge Regression is better in terms of yielding a lower RMSE than Lasso in this case. Test set RMSE is reasonably low. Best alpha was 0.3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoSJ_GayV2UU",
        "colab_type": "text"
      },
      "source": [
        "## e. \n",
        "Next, perform regression using Stochastic Gradient Descent for regression. For this part, you should use the SGDRegessor module from sklearn.linear_model. Again, start by a creating randomized 80%-20% train-test split. SGDRegessor requires that features be standardized (with 0 mean and scaled by standard deviation). Prior to fiting the model, perform the scaling using StandardScaler from sklearn.preprocessing. For this problem, perform a grid search (using GridSearchCV from sklearn.grid_search) Your grid search should compare combinations of two penalty parameters ('l2', 'l1') and different values of alpha (alpha could vary from 0.0001 which is the default to relatively large values, say 10). Using the best parameters, apply the model to the set-aside test data. Finally, perform model selection (similar to part d, above) to find the best \"l1_ratio\" parameter using SGDRegressor with  the \"elasticnet\" penalty parameter. [Note: \"l1_ratio\" is The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1;  l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1 penalty; defaults to 0.15.] Using the best mixing ratio, apply the Elastic Net model to the set-aside test data. Provide a summary of your findings from the above experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tfp2fNCfUFwK",
        "colab_type": "text"
      },
      "source": [
        "**80/20 train/test split**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TavyxoNtUqVr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Imports\n",
        "from sklearn.linear_model import SGDRegressor, ElasticNet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKP5WeMLUE-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Split the data into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(com_x_np, com_target_np, test_size=0.2, random_state=7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmPozZ-gUNhE",
        "colab_type": "text"
      },
      "source": [
        "*  Already standardized, but will use StandarScaler anyways."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV6Oi8BmUXxc",
        "colab_type": "text"
      },
      "source": [
        "**For this problem, perform a grid search (using GridSearchCV from sklearn.grid_search) Your grid search should compare combinations of two penalty parameters ('l2', 'l1') and different values of alpha (alpha could vary from 0.0001 which is the default to relatively large values, say 10).**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cdB6tKZZydn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "5955f2d2-1aff-4789-da1c-27941ac3b426"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "sgdreg = SGDRegressor()\n",
        "\n",
        "param_grid = {\n",
        "    'alpha': np.linspace(.0001, 10, 100),\n",
        "    'penalty': ['l2', 'l1'],\n",
        "}\n",
        "gridsearch = GridSearchCV(sgdreg, param_grid, cv=5)\n",
        "gridsearch.fit(X_train, y_train)"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
              "             estimator=SGDRegressor(alpha=0.0001, average=False,\n",
              "                                    early_stopping=False, epsilon=0.1,\n",
              "                                    eta0=0.01, fit_intercept=True,\n",
              "                                    l1_ratio=0.15, learning_rate='invscaling',\n",
              "                                    loss='squared_loss', max_iter=1000,\n",
              "                                    n_iter_no_change=5, penalty='l2',\n",
              "                                    power_t=0.25, random_state=None,\n",
              "                                    shuffle=True, tol=0.001,\n",
              "                                    validation_fraction=0.1, ver...\n",
              "       8.08082727e+00, 8.18183636e+00, 8.28284545e+00, 8.38385455e+00,\n",
              "       8.48486364e+00, 8.58587273e+00, 8.68688182e+00, 8.78789091e+00,\n",
              "       8.88890000e+00, 8.98990909e+00, 9.09091818e+00, 9.19192727e+00,\n",
              "       9.29293636e+00, 9.39394545e+00, 9.49495455e+00, 9.59596364e+00,\n",
              "       9.69697273e+00, 9.79798182e+00, 9.89899091e+00, 1.00000000e+01]),\n",
              "                         'penalty': ['l2', 'l1']},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 243
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxuDyz7mZ8Bp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a25d0a0f-1c06-4379-f85f-6ba742e347bf"
      },
      "source": [
        "print(\"Best Parameters: \" + str(gridsearch.best_params_))"
      ],
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Parameters: {'alpha': 0.0001, 'penalty': 'l1'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDz8RelJcVOd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a686af67-2bf2-4fe4-dc31-9715ee77f440"
      },
      "source": [
        "print(\"Best Score: \" + str(gridsearch.best_score_))"
      ],
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Score: 0.6274205126459339\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOBTNrrUZ6On",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0b2753d5-ee43-4694-a43e-38e49fab5311"
      },
      "source": [
        "# SGD is very senstitive to varying-sized feature values. So, first we need to do feature scaling. but it's already scaled\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "x_s = scaler.transform(X_train)\n",
        "\n",
        "sgdreg = SGDRegressor(penalty='l1', alpha=0.0001, max_iter=300)\n",
        "\n",
        "# Compute RMSE on training data\n",
        "sgdreg.fit(x_s,y_train)\n",
        "p = sgdreg.predict(x_s)\n",
        "err = p-y_train\n",
        "total_error = np.dot(err,err)\n",
        "rmse_train = np.sqrt(total_error/len(p))\n",
        "\n",
        "# Compute RMSE using 10-fold x-validation\n",
        "#kf = KFold(len(x), n_folds=10)\n",
        "kf = KFold(n_splits=10)\n",
        "kf.get_n_splits(X_train)\n",
        "\n",
        "KFold(n_splits=n,random_state=None, shuffle=False)\n",
        "    \n",
        "xval_err = 0\n",
        "for train,test in kf.split(X_train):\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(X_train[train])  \n",
        "    xtrain_s = scaler.transform(X_train[train])\n",
        "    xtest_s = scaler.transform(X_train[test])\n",
        "    sgdreg.fit(xtrain_s,y_train[train])\n",
        "    p = sgdreg.predict(xtest_s)\n",
        "    e = p-y_train[test]\n",
        "    xval_err += np.dot(e,e)\n",
        "rmse_10cv = np.sqrt(xval_err/len(X_train))\n",
        "\n",
        "method_name = 'Stochastic Gradient Descent Regression'\n",
        "print('Method: %s' %method_name)\n",
        "print('RMSE on training: %.4f' %rmse_train)\n",
        "print('RMSE on 5-fold CV: %.4f' %rmse_10cv)"
      ],
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Method: Stochastic Gradient Descent Regression\n",
            "RMSE on training: 0.1338\n",
            "RMSE on 5-fold CV: 0.1411\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqUTWHzXck8B",
        "colab_type": "text"
      },
      "source": [
        "**Finally, perform model selection (similar to part d, above) to find the best \"l1_ratio\" parameter using SGDRegressor with the \"elasticnet\" penalty parameter**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ep-z5YrFgxH9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5b26b42a-3299-413f-ca1e-f891c6d2e2f3"
      },
      "source": [
        "#best\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_test)\n",
        "x_t = scaler.transform(X_test)\n",
        "\n",
        "sgdreg = SGDRegressor(penalty='elasticnet', alpha=0.0001, max_iter=300)\n",
        "\n",
        "# Compute RMSE on training data\n",
        "sgdreg.fit(x_t,y_test)\n",
        "p = sgdreg.predict(x_t)\n",
        "err = p-y_test\n",
        "total_error = np.dot(err,err)\n",
        "rmse_train = np.sqrt(total_error/len(p))\n",
        "\n",
        "# Compute RMSE using 10-fold x-validation\n",
        "#kf = KFold(len(x), n_folds=10)\n",
        "kf = KFold(n_splits=10)\n",
        "kf.get_n_splits(X_test)\n",
        "\n",
        "KFold(n_splits=n,random_state=None, shuffle=False)\n",
        "    \n",
        "xval_err = 0\n",
        "for train,test in kf.split(X_test):\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(X_test[train])  # Don't cheat - fit only on training data\n",
        "    xtrain_s = scaler.transform(X_test[train])\n",
        "    xtest_s = scaler.transform(X_test[test])  # apply same transformation to test data\n",
        "    sgdreg.fit(xtrain_s,y_test[train])\n",
        "    p = sgdreg.predict(xtest_s)\n",
        "    e = p-y_test[test]\n",
        "    xval_err += np.dot(e,e)\n",
        "rmse_10cv = np.sqrt(xval_err/len(X_test))\n",
        "\n",
        "method_name = 'Elastic Net Regression'\n",
        "print('Method: %s' %method_name)\n",
        "print('RMSE on training: %.4f' %rmse_train)\n",
        "print('RMSE on 5-fold CV: %.4f' %rmse_10cv)"
      ],
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Method: Elastic Net Regression\n",
            "RMSE on training: 0.1130\n",
            "RMSE on 5-fold CV: 0.1334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNVmPtEYiDwE",
        "colab_type": "text"
      },
      "source": [
        "**Summary**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7vAuiidhMQC",
        "colab_type": "text"
      },
      "source": [
        "* Best is actually worse than for the other method from this run. \n",
        "\n",
        "*  Ridge Regresion with alpha = 0.3\n",
        " * RMSE on testing: 0.1081\n",
        " * RMSE on 5-fold CV: 0.1344\n",
        "\n",
        "*  Elastic Net Regression\n",
        " * RMSE on training: 0.1161\n",
        " * RMSE on 5-fold CV: 0.1332\n",
        "\n",
        "\n",
        "*  Simple Linear Regression\n",
        " * RMSE on training: 0.1289\n",
        " * RMSE on 5-fold CV: 0.1359\n",
        "\n",
        "\n",
        "*  Stochastic Gradient Descent Regression\n",
        " * RMSE on training: 0.1368\n",
        " * RMSE on 5-fold CV: 0.1399\n",
        "\n",
        "\n",
        "*  Lasso\n",
        " * RMSE on training: 0.2326\n",
        " * RMSE on 5-fold CV: 0.2327\n",
        "\n",
        "* It appears that Ridge Regression and Elastic net were the top performers. Lasso the worst.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAOqaeAeV2aB",
        "colab_type": "text"
      },
      "source": [
        "# Problem 2 - Automatic Document Clustering\n",
        "\n",
        "For this problem you will use a different subset of the 20 Newsgroup data set that you used in Assignment 2  (see the description of the full dataset). The subset for this assignment includes 2,500 documents (newsgroup posts), each belonging to one of 5 categories windows (0), crypt (1), christian (2), hockey (3), forsale (4). The documents are represented by 9328 terms (stems). The dictionary (vocabulary) for the data set is given in the file \"terms.txt\" and the full term-by-document matrix is given in \"matrix.txt\" (comma separated values). The actual category labels for the documents are provided in the file \"classes.txt\". Your goal in this assignment is to perform clustering on the documents and compare the clusters to the actual categories.\n",
        "\n",
        "Your tasks in this problem are the following [Note: for the clustering part of this assignment you should use the kMeans module form Ch. 10 of MLA (use the version provided here as it includes some corrections to the book version). You may also use Pandas and other modules from scikit-learn that you may need for preprocessing or evaluation.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S2TK0NKV2fv",
        "colab_type": "text"
      },
      "source": [
        "## a.\n",
        "\n",
        "Create your own distance function that, instead of using Euclidean distance, uses Cosine similarity. This is the distance function you will use to pass to the kMeans function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUI2CJmtZ5p9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#MLA cosine sim implementation\n",
        "import numpy as np\n",
        "def cosSim(inA,inB):\n",
        " num = float(inA.T*inB)\n",
        " denom = np.linalg.norm(inA)*np.linalg.norm(inB)\n",
        " return 0.5+0.5*(num/denom)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLcecY22DeDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def distEclud(vecA, vecB):\n",
        "  return np.sqrt(sum(np.power(vecA - vecB, 2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2K6juwL3_gG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#MLA kMeans implementation\n",
        "def loadDataSet(fileName):\n",
        "  dataMat = []\n",
        "  fr = open(fileName)\n",
        "  for line in fr.readlines():\n",
        "    curLine = line.strip().split('\\t')\n",
        "    fltLine = map(float,curLine)\n",
        "    dataMat.append(fltLine)\n",
        "  return dataMat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZFhKqh159cD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#MLA\n",
        "def randCent(dataSet, k):\n",
        "  n = np.shape(dataSet)[1]\n",
        "  centroids = np.mat(np.zeros((k,n)))\n",
        "  for j in range(n):\n",
        "    minJ = min(dataSet[:,j])\n",
        "    rangeJ = float(max(dataSet[:,j]) - minJ)\n",
        "    centroids[:,j] = minJ + rangeJ * np.random.rand(k,1)\n",
        "  return centroids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2SjqW3x6Bk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#MLA\n",
        "def MLA_kMeans(dataSet, k, distMeas=distEclud, createCent=randCent):\n",
        "  m = np.shape(dataSet)[0]\n",
        "  clusterAssment = np.mat(np.zeros((m,2)))\n",
        "  centroids = createCent(dataSet, k)\n",
        "  clusterChanged = True\n",
        "  while clusterChanged:\n",
        "    clusterChanged = False\n",
        "    for i in range(m):\n",
        "      minDist = np.inf; minIndex = -1\n",
        "      for j in range(k):\n",
        "        distJI = distMeas(centroids[j,:],dataSet[i,:])\n",
        "        if distJI < minDist:\n",
        "          minDist = distJI; minIndex = j\n",
        "        if clusterAssment[i,0] != minIndex: clusterChanged = True\n",
        "        clusterAssment[i,:] = minIndex,minDist**2\n",
        "    print(centroids)\n",
        "    for cent in range(k):\n",
        "      ptsInClust = dataSet[nonzero(clusterAssment[:,0].A==cent)[0]]\n",
        "      centroids[cent,:] = mean(ptsInClust, axis=0)\n",
        "  return centroids, clusterAssment"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soBIt2UeWbUF",
        "colab_type": "text"
      },
      "source": [
        "## b.\n",
        " \n",
        "Load the data set [Note: the data matrix provided has terms as rows and documents as columns. Since you will be clustering documents, you'll need to take the transpose of this matrix so that your main data matrix is a document x term matrix. In Numpy, you may use the \".T\" operation to obtain the transpose.] Then, split the data set (the document x term matrix) and set aside 20% for later use (see below). Use the 80% segment for clustering in the next part. The 20% portion must be a random subset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3Pre2mf9uhF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "48104ddd-b87c-4f49-988e-e712e13171ca"
      },
      "source": [
        "#Mount Google Drive to get files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqy4Ap-j8zu9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3cf07910-7f71-4a5b-bd35-a5a41129b473"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "#classes.txt\n",
        "classpath = '/content/drive/My Drive/Colab Notebooks/datasets/newsgroups5/classes.txt'\n",
        "class_data = pd.read_csv(classpath, sep=\" \", skiprows=1, index_col=0, header=None )\n",
        "class_data.shape"
      ],
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2500, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 254
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPMx4DVn-bB5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "7a53521c-3b9f-40ff-fc5f-203342cb7415"
      },
      "source": [
        "class_data.head()"
      ],
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   1\n",
              "0   \n",
              "0  0\n",
              "1  1\n",
              "2  1\n",
              "3  1\n",
              "4  2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 255
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPmo8bvqCVX3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aa4eb49e-0fb5-4f0b-95ae-d0d79dba6b18"
      },
      "source": [
        "#terms.txt\n",
        "termspath = '/content/drive/My Drive/Colab Notebooks/datasets/newsgroups5/terms.txt'\n",
        "terms_data = pd.read_csv(termspath, header=None)\n",
        "terms_data.shape"
      ],
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9328, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3C62yQz8CrbK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b40f88b1-efd5-4391-990c-250df2496009"
      },
      "source": [
        "terms_data.head()"
      ],
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>aa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>aargh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>aaron</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>aaronc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ab</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        0\n",
              "0      aa\n",
              "1   aargh\n",
              "2   aaron\n",
              "3  aaronc\n",
              "4      ab"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 257
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsAh858tJif5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d03dd4ad-93a0-4b47-8eaa-71307baa98f1"
      },
      "source": [
        "terms_d2 = pd.read_csv(termspath, header=None, names=['terms'])\n",
        "terms_d2.head()"
      ],
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>terms</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>aa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>aargh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>aaron</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>aaronc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ab</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    terms\n",
              "0      aa\n",
              "1   aargh\n",
              "2   aaron\n",
              "3  aaronc\n",
              "4      ab"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 258
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXHGBt6E-oXs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1035b115-7065-495e-bcf6-9a5478e287a6"
      },
      "source": [
        "#matrix.txt\n",
        "matrixpath = '/content/drive/My Drive/Colab Notebooks/datasets/newsgroups5/matrix.txt'\n",
        "matrix_data = pd.read_csv(matrixpath, sep=\",\", header=None)\n",
        "matrix_data.shape"
      ],
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9328, 2500)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 259
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSZfVOGr-oF3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "fa56edbe-4094-4134-9ac0-8f0f434616d7"
      },
      "source": [
        "matrix_data.head()"
      ],
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>2460</th>\n",
              "      <th>2461</th>\n",
              "      <th>2462</th>\n",
              "      <th>2463</th>\n",
              "      <th>2464</th>\n",
              "      <th>2465</th>\n",
              "      <th>2466</th>\n",
              "      <th>2467</th>\n",
              "      <th>2468</th>\n",
              "      <th>2469</th>\n",
              "      <th>2470</th>\n",
              "      <th>2471</th>\n",
              "      <th>2472</th>\n",
              "      <th>2473</th>\n",
              "      <th>2474</th>\n",
              "      <th>2475</th>\n",
              "      <th>2476</th>\n",
              "      <th>2477</th>\n",
              "      <th>2478</th>\n",
              "      <th>2479</th>\n",
              "      <th>2480</th>\n",
              "      <th>2481</th>\n",
              "      <th>2482</th>\n",
              "      <th>2483</th>\n",
              "      <th>2484</th>\n",
              "      <th>2485</th>\n",
              "      <th>2486</th>\n",
              "      <th>2487</th>\n",
              "      <th>2488</th>\n",
              "      <th>2489</th>\n",
              "      <th>2490</th>\n",
              "      <th>2491</th>\n",
              "      <th>2492</th>\n",
              "      <th>2493</th>\n",
              "      <th>2494</th>\n",
              "      <th>2495</th>\n",
              "      <th>2496</th>\n",
              "      <th>2497</th>\n",
              "      <th>2498</th>\n",
              "      <th>2499</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 2500 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   0     1     2     3     4     5     ...  2494  2495  2496  2497  2498  2499\n",
              "0     0     0     0     0     0     0  ...     0     0     0     0     0     0\n",
              "1     0     0     0     0     0     0  ...     0     0     0     0     0     0\n",
              "2     0     0     0     0     0     0  ...     0     0     0     0     0     0\n",
              "3     0     0     0     0     0     0  ...     0     0     0     0     0     0\n",
              "4     0     0     0     0     0     0  ...     0     0     0     0     0     0\n",
              "\n",
              "[5 rows x 2500 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 260
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7LPzAt7CS7O",
        "colab_type": "text"
      },
      "source": [
        "**Since you will be clustering documents, you'll need to take the transpose of this matrix so that your main data matrix is a document x term matrix. In Numpy, you may use the \".T\" operation to obtain the transpose.]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hy2ZmgI7C5eU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d221289f-7ff1-4f65-f678-84dd5923524d"
      },
      "source": [
        "import numpy as np\n",
        "x2_T = matrix_data.values.T\n",
        "x2_T.shape"
      ],
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2500, 9328)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 261
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMIckApjIe2x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8e48524a-d4b2-4eb9-f8a6-5cd5194109e2"
      },
      "source": [
        "y2 = class_data.values\n",
        "y2.shape"
      ],
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2500, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 262
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNWW3La4KqIO",
        "colab_type": "text"
      },
      "source": [
        "**Then, split the data set (the document x term matrix) and set aside 20% for later use (see below). Use the 80% segment for clustering in the next part. The 20% portion must be a random subset.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVNzkHt8Z5Ns",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4069f9c7-f18f-451c-b794-6fa1bc81b72e"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x2_train,x2_test,y2_train,y2_test=train_test_split(x2_T, y2, test_size=0.2, random_state=7)\n",
        "print(x2_train.shape,x2_test.shape,y2_train.shape,y2_test.shape)"
      ],
      "execution_count": 263,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2000, 9328) (500, 9328) (2000, 1) (500, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47USG8zVWfVx",
        "colab_type": "text"
      },
      "source": [
        "##c.\n",
        "\n",
        "Perform Kmeans clustering on the training data. Write a function to display the top N terms in each cluster along with the cluster DF values for each term and the size of the cluster. The cluster DF value for a term t in a cluster C is the percentage of docs in cluster C in which term t appears (so, if a cluster has 500 documents, and term \"game\" appears in 100 of those 500 documents, then DF value of \"game\" in that cluster is 0.2 or 20%). Sort the terms for each cluster in decreasing order of the DF percentage. Here is an example of how this output might look like (here the top 10 terms for 3 of the 5 clusters are displayed in decreasing order of cluster DF values, but the mean frequency from the cluster centroid is also shown). [Extra Credit: use your favorite third party tool, ideally with a Python based API, to create a word cloud for each cluster.]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9aX_1KxZ4xC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "26ef80ed-34ad-4384-fdbc-98f58f09c8e7"
      },
      "source": [
        "#step 1: get DT\n",
        "DT = x2_train\n",
        "DT.shape"
      ],
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 9328)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 264
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DK3e1THc7M-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#my attempt at implementing TFIDF\n",
        "DF = (np.array([(DT!=0).sum(0)]).T)\n",
        "NMatrix = (np.ones(np.shape(DT.T), dtype=float)*len(DT))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XESUjaqJ7aaU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "917a97d0-f855-4319-bfec-dfb1d00db037"
      },
      "source": [
        "NMatrix.shape"
      ],
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9328, 2000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 266
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcQTU--m77o9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Tried another method bc in class method wasn't working. this also doesn't work though\n",
        "IDF = np.log(np.divide(NMatrix + 1 , DF + 1)) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yHOmosM8CsS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "e11ca5ba-cfee-4adc-814f-c0501c3d64bc"
      },
      "source": [
        "IDF"
      ],
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6.65549219, 6.65549219, 6.65549219, ..., 6.65549219, 6.65549219,\n",
              "        6.65549219],\n",
              "       [6.80964287, 6.80964287, 6.80964287, ..., 6.80964287, 6.80964287,\n",
              "        6.80964287],\n",
              "       [5.60567006, 5.60567006, 5.60567006, ..., 5.60567006, 5.60567006,\n",
              "        5.60567006],\n",
              "       ...,\n",
              "       [7.50279005, 7.50279005, 7.50279005, ..., 7.50279005, 7.50279005,\n",
              "        7.50279005],\n",
              "       [7.50279005, 7.50279005, 7.50279005, ..., 7.50279005, 7.50279005,\n",
              "        7.50279005],\n",
              "       [6.99196442, 6.99196442, 6.99196442, ..., 6.99196442, 6.99196442,\n",
              "        6.99196442]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 268
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86q4e_d2_5a0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf = DT*IDF.T\n",
        "tfidf.shape\n",
        "thisisntworking = np.array(tfidf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSkUzGXEAEkn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "b0ab6cb6-70d9-48ab-971b-8297f9a6daa4"
      },
      "source": [
        "num_clusters = 5\n",
        "centroids, clusters = MLA_kMeans(thisisntworking, num_clusters)  ###Would really like some clarification on why this does not work"
      ],
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-270-7f1b37c497e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcentroids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLA_kMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthisisntworking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_clusters\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m###Would really like some clarification on why this does not work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-252-e172e8ec0aad>\u001b[0m in \u001b[0;36mMLA_kMeans\u001b[0;34m(dataSet, k, distMeas, createCent)\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mdistJI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistMeas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcentroids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataSet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mdistJI\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mminDist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m           \u001b[0mminDist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistJI\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mminIndex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclusterAssment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mminIndex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclusterChanged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wql3PRzdH2jF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Sklearn to get some solution\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "transformer = TfidfTransformer(smooth_idf=False)\n",
        "\n",
        "tfidf = transformer.fit_transform(DT)\n",
        "tfidf = tfidf.toarray()\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=5, max_iter=200, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgjqiLQvIcWT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "af007cca-258b-465b-928a-bcd60fc5c289"
      },
      "source": [
        "kmeans.fit(tfidf)"
      ],
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 1938.3742320751858\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 1915.486648537155\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 1910.4331402924213\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 1908.8747028328662\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 1908.63502273282\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 1908.5069173462675\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 1908.4709360955899\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 1908.4340987274973\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 1908.397135356622\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 9, inertia 1908.3607273077491\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 10, inertia 1908.3300784921387\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 11, inertia 1908.2890526505694\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 12, inertia 1908.256281095801\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 13, inertia 1908.2223040895856\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 14, inertia 1908.2114223089152\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 15, inertia 1908.2060306420356\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 16, inertia 1908.2014292432068\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 17, inertia 1908.2014292432068\n",
            "center shift 0.000000e+00 within tolerance 1.050925e-08\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 1934.0576827038092\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 1916.5372684771958\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 1912.6166647275766\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 1910.7076343117471\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 1909.6652449827393\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 1908.928135041756\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 1908.6724228347468\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 1908.5116555776738\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 1908.4474161250498\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 9, inertia 1908.412944616828\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 10, inertia 1908.3930472137476\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 11, inertia 1908.3670344661987\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 12, inertia 1908.3575820540525\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 13, inertia 1908.3479251557283\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 14, inertia 1908.3373581717653\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 15, inertia 1908.3041200154087\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 16, inertia 1908.2668771861515\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 17, inertia 1908.252573551687\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 18, inertia 1908.2274080818443\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 19, inertia 1908.2091378709295\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 20, inertia 1908.1624583288897\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 21, inertia 1908.1435441613273\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 22, inertia 1908.1388015161501\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 23, inertia 1908.1340810637244\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 24, inertia 1908.111228146828\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 25, inertia 1908.106681982426\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 26, inertia 1908.106681982426\n",
            "center shift 0.000000e+00 within tolerance 1.050925e-08\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 1935.182926526563\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 1918.083619034333\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 1912.1226811054155\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 1909.7905486518305\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 1909.08883774905\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 1908.6395976223698\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 1908.2201410506625\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 1907.8846242952766\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 1907.7791393779785\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 9, inertia 1907.7307196806369\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 10, inertia 1907.7136049075684\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 11, inertia 1907.6964720758249\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 12, inertia 1907.6918565167557\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 13, inertia 1907.6918565167557\n",
            "center shift 0.000000e+00 within tolerance 1.050925e-08\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 1934.5096319770855\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 1924.0173074392585\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 1920.9165360140646\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 1918.3889217646392\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 1917.9132776658746\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 1917.6255910193256\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 1917.5452472590669\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 1917.4853724659424\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 1917.4452104111385\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 9, inertia 1917.4301562822982\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 10, inertia 1917.4301562822982\n",
            "center shift 0.000000e+00 within tolerance 1.050925e-08\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 1936.3216819572278\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 1922.6269540538897\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 1919.0234148993302\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 1917.234332773737\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 1915.6293166486598\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 1913.6879747962087\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 1911.3483026460624\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 1910.5348829176207\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 1910.2734423378408\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 9, inertia 1909.9847987427004\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 10, inertia 1909.453052194301\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 11, inertia 1908.9391326133225\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 12, inertia 1908.6224404248592\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 13, inertia 1908.4635927431327\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 14, inertia 1908.3427250524792\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 15, inertia 1908.290292255765\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 16, inertia 1908.2653201811265\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 17, inertia 1908.2382913500896\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 18, inertia 1908.2143704897585\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 19, inertia 1908.2096895989646\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 20, inertia 1908.2096895989646\n",
            "center shift 0.000000e+00 within tolerance 1.050925e-08\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 1925.3488604623285\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 1918.1079478236907\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 1916.3990809953555\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 1915.858802502251\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 1915.6712053182316\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 1915.531570281372\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 1915.4626356780584\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 1915.4306213969257\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 1915.3899017286362\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 9, inertia 1915.341992046055\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 10, inertia 1915.3099074808024\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 11, inertia 1915.278458819724\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 12, inertia 1915.2593786317648\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 13, inertia 1915.230311045581\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 14, inertia 1915.2017690545458\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 15, inertia 1915.1949463193405\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 16, inertia 1915.1605174679466\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 17, inertia 1915.1455986141345\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 18, inertia 1915.1455986141345\n",
            "center shift 0.000000e+00 within tolerance 1.050925e-08\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 1936.862140106844\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 1924.241522770152\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 1920.7759047217464\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 1919.2925242030853\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 1918.3643893548608\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 1917.917758031468\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 1917.8113098794643\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 1917.6856490225848\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 1917.617574207434\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 9, inertia 1917.5741632437293\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 10, inertia 1917.545922325538\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 11, inertia 1917.5232759214737\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 12, inertia 1917.486470153241\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 13, inertia 1917.445622714723\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 14, inertia 1917.4329560823028\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 15, inertia 1917.4262398979054\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 16, inertia 1917.4191071364414\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 17, inertia 1917.4191071364414\n",
            "center shift 0.000000e+00 within tolerance 1.050925e-08\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 1937.6825836696876\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 1918.4261908158783\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 1915.5695299986241\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 1915.0828342649736\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 1914.9186466270546\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 1914.8072791965137\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 1914.7384282097794\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 1914.684852792301\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 1914.6510816947023\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 9, inertia 1914.6510816947023\n",
            "center shift 0.000000e+00 within tolerance 1.050925e-08\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 1935.1193028785663\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 1915.315539346398\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 1910.4394755530802\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 1909.0426500107758\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 1908.3243060345696\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 1907.8843956554883\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 1907.8047921150308\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 1907.77599188135\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 1907.7670208877535\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 9, inertia 1907.7574016396577\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 10, inertia 1907.7519916253655\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 11, inertia 1907.7519916253655\n",
            "center shift 0.000000e+00 within tolerance 1.050925e-08\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 1938.0687809829851\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 1920.9626770562131\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 1918.842714993896\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 1918.5491860053835\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 1918.353901413454\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 1918.254631201095\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 1918.1661315948913\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 1918.1549775822336\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 1918.1473041008578\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 9, inertia 1918.1366222085974\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 10, inertia 1918.128926586914\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 11, inertia 1918.1005036056122\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 12, inertia 1918.068578450636\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 13, inertia 1918.0397204544306\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 14, inertia 1918.0397204544306\n",
            "center shift 0.000000e+00 within tolerance 1.050925e-08\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=200,\n",
              "       n_clusters=5, n_init=10, n_jobs=None, precompute_distances='auto',\n",
              "       random_state=None, tol=0.0001, verbose=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 272
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQDQ_zwmIvmX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5abf062e-e483-40e7-970f-5d57e72c85fc"
      },
      "source": [
        "clusters = kmeans.predict(tfidf)\n",
        "print(clusters)"
      ],
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3 0 0 ... 3 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PjLpv2KI4qW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def top_N(df, clusters, k, n):\n",
        "    for i in range(k):\n",
        "        tfidf_df = pd.DataFrame(df, columns=terms_d2.terms)\n",
        "        indcluster = tfidf_df[clusters[0:]==i]\n",
        "        #DocFrequencyDF\n",
        "        DF = (indcluster!=0).sum(0)\n",
        "        #calc DF/total docs in cluster x\n",
        "        percent_cluster = DF/float(indcluster.shape[0])\n",
        "        #solve\n",
        "        getvals = percent_cluster.sort_values(ascending=False).head(n).to_frame()\n",
        "        getvals.columns = ['percentage']\n",
        "        getvals['counts'] = DF.sort_values(ascending=False).head(n).to_frame()\n",
        "\n",
        "        print(\"Number of documents in cluster: \", indcluster.shape[0])\n",
        "        print('{:<15s}{:<15s}{:<15s}{:<15s}'.format('word','frequency','DF','PercentofDocs'))\n",
        "        for j in range(n):\n",
        "          print('{:<15}{:<15}{:<15}{:<15}'.format(getvals.index.values[j],'na',getvals['counts'][j],round(getvals['percentage'][j]*100,6)))\n",
        "        print (\"--------------------------------------------------------------\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kD2dBOlJHKB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a3557346-f2fd-4c26-a4e3-e8b87b7a6acb"
      },
      "source": [
        "#input matrix, clusters calculated above, 5 clusters, and top 10 words in each cluster\n",
        "top_N(tfidf, clusters, 5, 10)"
      ],
      "execution_count": 275,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of documents in cluster:  334\n",
            "word           frequency      DF             PercentofDocs  \n",
            "subject        na             334            100.0          \n",
            "game           na             208            62.275449      \n",
            "write          na             185            55.389222      \n",
            "team           na             171            51.197605      \n",
            "plai           na             160            47.904192      \n",
            "go             na             152            45.508982      \n",
            "articl         na             151            45.209581      \n",
            "hockei         na             145            43.413174      \n",
            "on             na             137            41.017964      \n",
            "get            na             126            37.724551      \n",
            "--------------------------------------------------------------\n",
            "Number of documents in cluster:  315\n",
            "word           frequency      DF             PercentofDocs  \n",
            "subject        na             315            100.0          \n",
            "window         na             268            85.079365      \n",
            "write          na             148            46.984127      \n",
            "file           na             129            40.952381      \n",
            "do             na             116            36.825397      \n",
            "run            na             114            36.190476      \n",
            "know           na             108            34.285714      \n",
            "work           na             104            33.015873      \n",
            "get            na             104            33.015873      \n",
            "on             na             103            32.698413      \n",
            "--------------------------------------------------------------\n",
            "Number of documents in cluster:  282\n",
            "word           frequency      DF             PercentofDocs  \n",
            "subject        na             282            100.0          \n",
            "god            na             206            73.049645      \n",
            "write          na             194            68.794326      \n",
            "on             na             178            63.120567      \n",
            "peopl          na             157            55.673759      \n",
            "christian      na             155            54.964539      \n",
            "think          na             145            51.41844       \n",
            "articl         na             144            51.06383       \n",
            "believ         na             141            50.0           \n",
            "know           na             135            47.87234       \n",
            "--------------------------------------------------------------\n",
            "Number of documents in cluster:  789\n",
            "word           frequency      DF             PercentofDocs  \n",
            "subject        na             789            100.0          \n",
            "write          na             238            30.164766      \n",
            "sale           na             233            29.531052      \n",
            "on             na             217            27.503169      \n",
            "email          na             207            26.235741      \n",
            "pleas          na             184            23.320659      \n",
            "articl         na             169            21.419518      \n",
            "get            na             157            19.898606      \n",
            "know           na             151            19.13815       \n",
            "thank          na             144            18.250951      \n",
            "--------------------------------------------------------------\n",
            "Number of documents in cluster:  280\n",
            "word           frequency      DF             PercentofDocs  \n",
            "subject        na             280            100.0          \n",
            "write          na             187            66.785714      \n",
            "kei            na             176            62.857143      \n",
            "clipper        na             163            58.214286      \n",
            "encrypt        na             158            56.428571      \n",
            "chip           na             152            54.285714      \n",
            "articl         na             148            52.857143      \n",
            "on             na             138            49.285714      \n",
            "govern         na             138            49.285714      \n",
            "secur          na             109            38.928571      \n",
            "--------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovuilFMVWfb-",
        "colab_type": "text"
      },
      "source": [
        "## d. \n",
        "\n",
        "Using the cluster assignments from Kmeans clustering, compare your 5 clusters to the 5 pre-assigned classes by computing the Completeness and Homogeneity values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ko81-C4FTFYe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "5bc463fa-4974-455c-ac86-bbec83d8b80f"
      },
      "source": [
        "y2_train.shape\n",
        "import pandas as pd\n",
        "gety2train = pd.DataFrame(y2_train)\n",
        "gety2train.head()"
      ],
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0\n",
              "0  4\n",
              "1  3\n",
              "2  3\n",
              "3  0\n",
              "4  1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 276
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojqK3blfZ4Xd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7979ee4f-1e11-444b-f88c-180838628582"
      },
      "source": [
        "#computing the Completeness and Homogeneity values of the generated clusters\n",
        "from sklearn.metrics import completeness_score, homogeneity_score\n",
        "\n",
        "print(\"completeness_score\",completeness_score(gety2train[0],clusters))\n",
        "print(\"homogeneity_score\",homogeneity_score(gety2train[0],clusters))"
      ],
      "execution_count": 277,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "completeness_score 0.6696026636593171\n",
            "homogeneity_score 0.627591370689204\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdZJM7eBWfhS",
        "colab_type": "text"
      },
      "source": [
        "## e.\n",
        "\n",
        "Finally, using your cluster assignments as class labels, categorize each of the documents in the 20% set-aside data into each of the appropriate cluster. Your categorization should be based on Cosine similarity between each test document and cluster centroids. For each test document show the predicted class label as well as Cosine similarity to the corresponding cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wrt-TssVwUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#predict on the test set\n",
        "tfidftest_test = transformer.fit_transform(x2_test)\n",
        "tfidftest_test = tfidftest_test.toarray()\n",
        "\n",
        "cluster_test = kmeans.predict(tfidftest_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WQOfrwuW9N5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "439703a7-4f5b-4bc3-926e-6f5b1f33a4f1"
      },
      "source": [
        "y2_test.shape\n",
        "import pandas as pd\n",
        "gety2test = pd.DataFrame(y2_test)\n",
        "gety2test.head()"
      ],
      "execution_count": 279,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0\n",
              "0  0\n",
              "1  1\n",
              "2  1\n",
              "3  3\n",
              "4  4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 279
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKP8hBLoWday",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "82527832-f2f7-41e5-da46-2fc630b84713"
      },
      "source": [
        "#not clear on what the question is asking for but:\n",
        "#computing the Completeness and Homogeneity values of the generated clusters\n",
        "from sklearn.metrics import completeness_score, homogeneity_score\n",
        "\n",
        "print(\"completeness_score\",completeness_score(gety2test[0],cluster_test))\n",
        "print(\"homogeneity_score\",homogeneity_score(gety2test[0],cluster_test))"
      ],
      "execution_count": 280,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "completeness_score 0.7210223760534522\n",
            "homogeneity_score 0.6843941450365368\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xu1fjQhWtQq",
        "colab_type": "text"
      },
      "source": [
        "# Problem 3 - PCA for Reduced Dimensionality in Clustering\n",
        "\n",
        "For this problem you will use an image segmentation data set for clustering. You will experiment with using PCA as an approach to reduce dimensionality and noise in the data. You will compare the results of clustering the data with and without PCA using the provided image class assignments as the ground truth. The data set is divided into three files. The file \"segmentation_data.txt\" contains data about images with each line corresponding to one image. Each image is represented by 19 features (these are the columns in the data and correspond to the feature names in the file \"segmentation_names.txt\". The file \"segmentation_classes.txt\" contains the class labels (the type of image) and a numeric class label for each of the corresponding images in the data file. After clustering the image data, you will use the class labels to measure completeness and homogeneity of the generated clusters. The data set used in this problem is based on the Image Segmentation data set at the UCI Machine Learning Repository.\n",
        "\n",
        "Your tasks in this problem are the following:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFiTeLOJWtWe",
        "colab_type": "text"
      },
      "source": [
        "## a. \n",
        "\n",
        "Load in the image data matrix (with rows as images and columns as features). Also load in the numeric class labels from the segmentation class file. Using your favorite method (e.g., sklearn's min-max scaler), perform min-max normalization on the data matrix so that each feature is scaled to [0,1] range."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OObRoEGCAXyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#imports\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZU85FTcZ3TZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bfc46b1e-62c5-4b9f-efd1-5e9fd9ec3337"
      },
      "source": [
        "#Mount Google Drive to get files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 282,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRrVgwYcAXGa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7ee3aadc-ff2e-41dd-af55-06007f59db1a"
      },
      "source": [
        "#image data matrix\n",
        "path1 = '/content/drive/My Drive/Colab Notebooks/datasets/segmentation_data/segmentation_data.txt'\n",
        "segmentation_data = pd.read_csv(path1, sep=\",\", header=None)\n",
        "segmentation_data.shape"
      ],
      "execution_count": 283,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2100, 19)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 283
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKINuTvLEeSI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "ae525b86-9c8b-4ecc-96a4-b78cc4d61507"
      },
      "source": [
        "#check rows as images and columns as features\n",
        "segmentation_data.head()"
      ],
      "execution_count": 284,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>110.0</td>\n",
              "      <td>189.0</td>\n",
              "      <td>9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>1.222222</td>\n",
              "      <td>1.186342</td>\n",
              "      <td>12.925926</td>\n",
              "      <td>10.888889</td>\n",
              "      <td>9.222222</td>\n",
              "      <td>18.666668</td>\n",
              "      <td>-6.111111</td>\n",
              "      <td>-11.111111</td>\n",
              "      <td>17.222221</td>\n",
              "      <td>18.666668</td>\n",
              "      <td>0.508139</td>\n",
              "      <td>1.910864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>86.0</td>\n",
              "      <td>187.0</td>\n",
              "      <td>9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.111111</td>\n",
              "      <td>0.720082</td>\n",
              "      <td>1.444444</td>\n",
              "      <td>0.750309</td>\n",
              "      <td>13.740741</td>\n",
              "      <td>11.666667</td>\n",
              "      <td>10.333334</td>\n",
              "      <td>19.222221</td>\n",
              "      <td>-6.222222</td>\n",
              "      <td>-10.222222</td>\n",
              "      <td>16.444445</td>\n",
              "      <td>19.222221</td>\n",
              "      <td>0.463329</td>\n",
              "      <td>1.941465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>225.0</td>\n",
              "      <td>244.0</td>\n",
              "      <td>9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.388889</td>\n",
              "      <td>2.195113</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.520234</td>\n",
              "      <td>12.259259</td>\n",
              "      <td>10.333334</td>\n",
              "      <td>9.333334</td>\n",
              "      <td>17.111110</td>\n",
              "      <td>-5.777778</td>\n",
              "      <td>-8.777778</td>\n",
              "      <td>14.555555</td>\n",
              "      <td>17.111110</td>\n",
              "      <td>0.480149</td>\n",
              "      <td>1.987902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>47.0</td>\n",
              "      <td>232.0</td>\n",
              "      <td>9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.277778</td>\n",
              "      <td>1.254621</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.894427</td>\n",
              "      <td>12.703704</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>18.111110</td>\n",
              "      <td>-5.111111</td>\n",
              "      <td>-11.111111</td>\n",
              "      <td>16.222221</td>\n",
              "      <td>18.111110</td>\n",
              "      <td>0.500966</td>\n",
              "      <td>1.875362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>97.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.166667</td>\n",
              "      <td>0.691215</td>\n",
              "      <td>1.166667</td>\n",
              "      <td>1.005540</td>\n",
              "      <td>15.592592</td>\n",
              "      <td>13.888889</td>\n",
              "      <td>11.777778</td>\n",
              "      <td>21.111110</td>\n",
              "      <td>-5.111111</td>\n",
              "      <td>-11.444445</td>\n",
              "      <td>16.555555</td>\n",
              "      <td>21.111110</td>\n",
              "      <td>0.442661</td>\n",
              "      <td>1.863654</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      0      1   2    3   ...         15         16        17        18\n",
              "0  110.0  189.0   9  0.0  ...  17.222221  18.666668  0.508139  1.910864\n",
              "1   86.0  187.0   9  0.0  ...  16.444445  19.222221  0.463329  1.941465\n",
              "2  225.0  244.0   9  0.0  ...  14.555555  17.111110  0.480149  1.987902\n",
              "3   47.0  232.0   9  0.0  ...  16.222221  18.111110  0.500966  1.875362\n",
              "4   97.0  186.0   9  0.0  ...  16.555555  21.111110  0.442661  1.863654\n",
              "\n",
              "[5 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 284
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-Y3ul04F_Oi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "c6de0f6b-339d-496a-e54b-256adc8f212d"
      },
      "source": [
        "#Add in feature labels\n",
        "path3 = '/content/drive/My Drive/Colab Notebooks/datasets/segmentation_data/segmentation_names.txt'\n",
        "names = pd.read_csv(path3, sep=\",\", header=None)\n",
        "names.columns = ['names']\n",
        "segmentation_data.columns = [names.names.tolist()]\n",
        "segmentation_data.head()"
      ],
      "execution_count": 285,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>REGION-CENTROID-COL</th>\n",
              "      <th>REGION-CENTROID-ROW</th>\n",
              "      <th>REGION-PIXEL-COUNT</th>\n",
              "      <th>SHORT-LINE-DENSITY-5</th>\n",
              "      <th>SHORT-LINE-DENSITY-2</th>\n",
              "      <th>VEDGE-MEAN</th>\n",
              "      <th>VEDGE-SD</th>\n",
              "      <th>HEDGE-MEAN</th>\n",
              "      <th>HEDGE-SD</th>\n",
              "      <th>INTENSITY-MEAN</th>\n",
              "      <th>RAWRED-MEAN</th>\n",
              "      <th>RAWBLUE-MEAN</th>\n",
              "      <th>RAWGREEN-MEAN</th>\n",
              "      <th>EXRED-MEAN</th>\n",
              "      <th>EXBLUE-MEAN</th>\n",
              "      <th>EXGREEN-MEAN</th>\n",
              "      <th>VALUE-MEAN</th>\n",
              "      <th>SATURATION-MEAN</th>\n",
              "      <th>HUE-MEAN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>110.0</td>\n",
              "      <td>189.0</td>\n",
              "      <td>9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>1.222222</td>\n",
              "      <td>1.186342</td>\n",
              "      <td>12.925926</td>\n",
              "      <td>10.888889</td>\n",
              "      <td>9.222222</td>\n",
              "      <td>18.666668</td>\n",
              "      <td>-6.111111</td>\n",
              "      <td>-11.111111</td>\n",
              "      <td>17.222221</td>\n",
              "      <td>18.666668</td>\n",
              "      <td>0.508139</td>\n",
              "      <td>1.910864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>86.0</td>\n",
              "      <td>187.0</td>\n",
              "      <td>9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.111111</td>\n",
              "      <td>0.720082</td>\n",
              "      <td>1.444444</td>\n",
              "      <td>0.750309</td>\n",
              "      <td>13.740741</td>\n",
              "      <td>11.666667</td>\n",
              "      <td>10.333334</td>\n",
              "      <td>19.222221</td>\n",
              "      <td>-6.222222</td>\n",
              "      <td>-10.222222</td>\n",
              "      <td>16.444445</td>\n",
              "      <td>19.222221</td>\n",
              "      <td>0.463329</td>\n",
              "      <td>1.941465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>225.0</td>\n",
              "      <td>244.0</td>\n",
              "      <td>9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.388889</td>\n",
              "      <td>2.195113</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.520234</td>\n",
              "      <td>12.259259</td>\n",
              "      <td>10.333334</td>\n",
              "      <td>9.333334</td>\n",
              "      <td>17.111110</td>\n",
              "      <td>-5.777778</td>\n",
              "      <td>-8.777778</td>\n",
              "      <td>14.555555</td>\n",
              "      <td>17.111110</td>\n",
              "      <td>0.480149</td>\n",
              "      <td>1.987902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>47.0</td>\n",
              "      <td>232.0</td>\n",
              "      <td>9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.277778</td>\n",
              "      <td>1.254621</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.894427</td>\n",
              "      <td>12.703704</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>18.111110</td>\n",
              "      <td>-5.111111</td>\n",
              "      <td>-11.111111</td>\n",
              "      <td>16.222221</td>\n",
              "      <td>18.111110</td>\n",
              "      <td>0.500966</td>\n",
              "      <td>1.875362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>97.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.166667</td>\n",
              "      <td>0.691215</td>\n",
              "      <td>1.166667</td>\n",
              "      <td>1.005540</td>\n",
              "      <td>15.592592</td>\n",
              "      <td>13.888889</td>\n",
              "      <td>11.777778</td>\n",
              "      <td>21.111110</td>\n",
              "      <td>-5.111111</td>\n",
              "      <td>-11.444445</td>\n",
              "      <td>16.555555</td>\n",
              "      <td>21.111110</td>\n",
              "      <td>0.442661</td>\n",
              "      <td>1.863654</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  REGION-CENTROID-COL REGION-CENTROID-ROW  ... SATURATION-MEAN  HUE-MEAN\n",
              "0               110.0               189.0  ...        0.508139  1.910864\n",
              "1                86.0               187.0  ...        0.463329  1.941465\n",
              "2               225.0               244.0  ...        0.480149  1.987902\n",
              "3                47.0               232.0  ...        0.500966  1.875362\n",
              "4                97.0               186.0  ...        0.442661  1.863654\n",
              "\n",
              "[5 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 285
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vA9idkEWGcbz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        },
        "outputId": "bacd1542-64cc-455b-b502-97dbd0347306"
      },
      "source": [
        "#quick descriptive check\n",
        "segmentation_data.describe().T"
      ],
      "execution_count": 286,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>REGION-CENTROID-COL</th>\n",
              "      <td>2100.0</td>\n",
              "      <td>124.940476</td>\n",
              "      <td>72.858637</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>62.000000</td>\n",
              "      <td>121.000000</td>\n",
              "      <td>188.250000</td>\n",
              "      <td>254.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>REGION-CENTROID-ROW</th>\n",
              "      <td>2100.0</td>\n",
              "      <td>123.483333</td>\n",
              "      <td>57.431428</td>\n",
              "      <td>1.100000e+01</td>\n",
              "      <td>81.000000</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>171.250000</td>\n",
              "      <td>251.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>REGION-PIXEL-COUNT</th>\n",
              "      <td>2100.0</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>9.000000e+00</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SHORT-LINE-DENSITY-5</th>\n",
              "      <td>2100.0</td>\n",
              "      <td>0.014921</td>\n",
              "      <td>0.041024</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SHORT-LINE-DENSITY-2</th>\n",
              "      <td>2100.0</td>\n",
              "      <td>0.004550</td>\n",
              "      <td>0.023573</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.222222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>VEDGE-MEAN</th>\n",
              "      <td>2100.0</td>\n",
              "      <td>1.890820</td>\n",
              "      <td>2.649453</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.722222</td>\n",
              "      <td>1.277776</td>\n",
              "      <td>2.222221</td>\n",
              "      <td>29.222221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>VEDGE-SD</th>\n",
              "      <td>2100.0</td>\n",
              "      <td>5.708299</td>\n",
              "      <td>44.989359</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.349603</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>1.807406</td>\n",
              "      <td>991.718400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HEDGE-MEAN</th>\n",
              "      <td>2100.0</td>\n",
              "      <td>2.406772</td>\n",
              "      <td>3.469954</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.833332</td>\n",
              "      <td>1.444444</td>\n",
              "      <td>2.555556</td>\n",
              "      <td>44.722225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HEDGE-SD</th>\n",
              "      <td>2100.0</td>\n",
              "      <td>7.904224</td>\n",
              "      <td>53.471074</td>\n",
              "      <td>-1.589457e-08</td>\n",
              "      <td>0.421638</td>\n",
              "      <td>0.989744</td>\n",
              "      <td>2.251852</td>\n",
              "      <td>1386.329200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>INTENSITY-MEAN</th>\n",
              "      <td>2100.0</td>\n",
              "      <td>37.047654</td>\n",
              "      <td>38.135291</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>7.472222</td>\n",
              "      <td>21.666666</td>\n",
              "      <td>53.277778</td>\n",
              "      <td>143.444440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RAWRED-MEAN</th>\n",
              "      <td>2100.0</td>\n",
              "      <td>32.806667</td>\n",
              "      <td>34.994538</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>19.666668</td>\n",
              "      <td>47.333332</td>\n",
              "      <td>137.111110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RAWBLUE-MEAN</th>\n",
              "      <td>2100.0</td>\n",
              "      <td>44.205556</td>\n",
              "      <td>43.510119</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>9.666667</td>\n",
              "      <td>27.777779</td>\n",
              "      <td>65.000000</td>\n",
              "      <td>150.888890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RAWGREEN-MEAN</th>\n",
              "      <td>2100.0</td>\n",
              "      <td>34.130741</td>\n",
              "      <td>36.303768</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>6.222222</td>\n",
              "      <td>20.444445</td>\n",
              "      <td>46.388888</td>\n",
              "      <td>142.555560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>EXRED-MEAN</th>\n",
              "      <td>2100.0</td>\n",
              "      <td>-12.722963</td>\n",
              "      <td>11.588214</td>\n",
              "      <td>-4.966667e+01</td>\n",
              "      <td>-18.583333</td>\n",
              "      <td>-10.888889</td>\n",
              "      <td>-4.222222</td>\n",
              "      <td>9.888889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>EXBLUE-MEAN</th>\n",
              "      <td>2100.0</td>\n",
              "      <td>21.473704</td>\n",
              "      <td>19.654107</td>\n",
              "      <td>-1.244444e+01</td>\n",
              "      <td>4.305556</td>\n",
              "      <td>19.666666</td>\n",
              "      <td>36.111110</td>\n",
              "      <td>82.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>EXGREEN-MEAN</th>\n",
              "      <td>2100.0</td>\n",
              "      <td>-8.750741</td>\n",
              "      <td>11.606996</td>\n",
              "      <td>-3.388889e+01</td>\n",
              "      <td>-17.000000</td>\n",
              "      <td>-11.000000</td>\n",
              "      <td>-3.222222</td>\n",
              "      <td>24.666666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>VALUE-MEAN</th>\n",
              "      <td>2100.0</td>\n",
              "      <td>45.162381</td>\n",
              "      <td>42.900582</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>11.777778</td>\n",
              "      <td>28.666666</td>\n",
              "      <td>65.000000</td>\n",
              "      <td>150.888890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SATURATION-MEAN</th>\n",
              "      <td>2100.0</td>\n",
              "      <td>0.427259</td>\n",
              "      <td>0.228458</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.284934</td>\n",
              "      <td>0.375064</td>\n",
              "      <td>0.540228</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HUE-MEAN</th>\n",
              "      <td>2100.0</td>\n",
              "      <td>-1.365147</td>\n",
              "      <td>1.544278</td>\n",
              "      <td>-3.044175e+00</td>\n",
              "      <td>-2.188539</td>\n",
              "      <td>-2.052625</td>\n",
              "      <td>-1.565745</td>\n",
              "      <td>2.912480</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       count        mean  ...         75%          max\n",
              "REGION-CENTROID-COL   2100.0  124.940476  ...  188.250000   254.000000\n",
              "REGION-CENTROID-ROW   2100.0  123.483333  ...  171.250000   251.000000\n",
              "REGION-PIXEL-COUNT    2100.0    9.000000  ...    9.000000     9.000000\n",
              "SHORT-LINE-DENSITY-5  2100.0    0.014921  ...    0.000000     0.333333\n",
              "SHORT-LINE-DENSITY-2  2100.0    0.004550  ...    0.000000     0.222222\n",
              "VEDGE-MEAN            2100.0    1.890820  ...    2.222221    29.222221\n",
              "VEDGE-SD              2100.0    5.708299  ...    1.807406   991.718400\n",
              "HEDGE-MEAN            2100.0    2.406772  ...    2.555556    44.722225\n",
              "HEDGE-SD              2100.0    7.904224  ...    2.251852  1386.329200\n",
              "INTENSITY-MEAN        2100.0   37.047654  ...   53.277778   143.444440\n",
              "RAWRED-MEAN           2100.0   32.806667  ...   47.333332   137.111110\n",
              "RAWBLUE-MEAN          2100.0   44.205556  ...   65.000000   150.888890\n",
              "RAWGREEN-MEAN         2100.0   34.130741  ...   46.388888   142.555560\n",
              "EXRED-MEAN            2100.0  -12.722963  ...   -4.222222     9.888889\n",
              "EXBLUE-MEAN           2100.0   21.473704  ...   36.111110    82.000000\n",
              "EXGREEN-MEAN          2100.0   -8.750741  ...   -3.222222    24.666666\n",
              "VALUE-MEAN            2100.0   45.162381  ...   65.000000   150.888890\n",
              "SATURATION-MEAN       2100.0    0.427259  ...    0.540228     1.000000\n",
              "HUE-MEAN              2100.0   -1.365147  ...   -1.565745     2.912480\n",
              "\n",
              "[19 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 286
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af4EFXMaE7tE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b73cd9f8-8cc0-4f06-9c39-2eb0158c6814"
      },
      "source": [
        "#also load the numeric class labels:\n",
        "path2 = '/content/drive/My Drive/Colab Notebooks/datasets/segmentation_data/segmentation_classes.txt'\n",
        "numeric_class_labels = pd.read_csv(path2, sep='\\t', header=None)\n",
        "numeric_class_labels.shape"
      ],
      "execution_count": 287,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2100, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 287
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktid6ac2FYd5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "31eec2e3-8ecb-426f-e969-16514367df25"
      },
      "source": [
        "numeric_class_labels.head()"
      ],
      "execution_count": 288,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>GRASS</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRASS</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>GRASS</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>GRASS</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>GRASS</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       0  1\n",
              "0  GRASS  0\n",
              "1  GRASS  0\n",
              "2  GRASS  0\n",
              "3  GRASS  0\n",
              "4  GRASS  0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 288
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyv-noRxFqxl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Using sklearn's min-max scaler\n",
        "from sklearn import preprocessing\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "\n",
        "seg_np = segmentation_data.values\n",
        "x = min_max_scaler.fit_transform(seg_np)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKsRmzq9HEk4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a1b13f58-3c15-423d-9f42-76f4a9ad75b0"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2100, 19)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 290
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpzGCDljHJAS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "64deed66-811c-4aaa-eac3-37d07777fbf0"
      },
      "source": [
        "x[:2]"
      ],
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4.30830040e-01, 7.41666667e-01, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 3.42205474e-02, 6.72233922e-04, 2.73291926e-02,\n",
              "        8.55743510e-04, 9.01110284e-02, 7.94165331e-02, 6.11192912e-02,\n",
              "        1.30943107e-01, 7.31343290e-01, 1.41176540e-02, 8.72865267e-01,\n",
              "        1.23711348e-01, 5.08138840e-01, 8.31849232e-01],\n",
              "       [3.35968379e-01, 7.33333333e-01, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 3.80228046e-02, 7.26095734e-04, 3.22981359e-02,\n",
              "        5.41219947e-04, 9.57913810e-02, 8.50891441e-02, 6.84830672e-02,\n",
              "        1.34840205e-01, 7.29477615e-01, 2.35294199e-02, 8.59582565e-01,\n",
              "        1.27393216e-01, 4.63329080e-01, 8.36986460e-01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 291
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIwlp145WtmF",
        "colab_type": "text"
      },
      "source": [
        "## b. \n",
        "\n",
        "Next, Perform Kmeans clustering (for this problem, use the Kmeans implementation in scikit-learn) on the image data (since there are a total 7 pre-assigned image classes, you should use K = 7 in your clustering). Use Euclidean distance as your distance measure for the clustering. Print the cluster centroids (use some formatting so that they are visually understandable). Compare your 7 clusters to the 7 pre-assigned classes by computing the Completeness and Homogeneity values of the generated clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9I2zQ7NZ2Mv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "626117c8-c2c9-4e33-a9e2-845c95a83fc1"
      },
      "source": [
        "from sklearn.cluster import KMeans \n",
        "\n",
        "kmeans = KMeans(n_clusters=7, max_iter=500, verbose=1) # initialization\n",
        "kmeans.fit(x)\n",
        "clusters = kmeans.predict(x)"
      ],
      "execution_count": 312,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 391.7977077837103\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 378.9753503862577\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 373.4191145838438\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 371.73553919206705\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 371.2847547309739\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 371.0386888977893\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 370.82733105721104\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 370.61717505677194\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 370.4750769307059\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 9, inertia 370.36959786911643\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 10, inertia 370.19726515036336\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 11, inertia 369.9424262285936\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 12, inertia 369.6981322340004\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 13, inertia 369.2497795216563\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 14, inertia 368.64745683522625\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 15, inertia 367.4575536008912\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 16, inertia 364.9455827674764\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 17, inertia 360.21565099085757\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 18, inertia 354.3619322914944\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 19, inertia 351.189043036712\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 20, inertia 350.32312168097417\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 21, inertia 350.0736501692719\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 22, inertia 350.0144204826619\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 23, inertia 350.01207974106956\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 24, inertia 350.01207974106956\n",
            "center shift 0.000000e+00 within tolerance 4.150157e-06\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 391.484965666262\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 375.4499767771164\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 372.9995429141642\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 371.5029357572425\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 370.788606161351\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 370.19012462487905\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 369.92768578422385\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 369.80007862199966\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 369.7396876970084\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 9, inertia 369.6892551363576\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 10, inertia 369.67889664848747\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 11, inertia 369.6676353909055\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 12, inertia 369.6631374214163\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 13, inertia 369.6581070531123\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 14, inertia 369.6504511676116\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 15, inertia 369.6504511676116\n",
            "center shift 0.000000e+00 within tolerance 4.150157e-06\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 409.4818207837038\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 402.1294884918049\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 397.18948941503595\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 393.45580276178435\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 391.01592643947623\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 389.92622293488427\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 389.46124385885946\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 388.64194747726884\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 387.1426686579563\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 9, inertia 384.0273830128832\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 10, inertia 380.31274084616143\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 11, inertia 377.53178124168977\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 12, inertia 376.29413810544946\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 13, inertia 375.1586258442936\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 14, inertia 374.49211329775227\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 15, inertia 374.15684037627847\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 16, inertia 374.0343171238228\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 17, inertia 373.98281881090975\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 18, inertia 373.9565949718484\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 19, inertia 373.9501023233352\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 20, inertia 373.94938189262257\n",
            "center shift 1.634761e-03 within tolerance 4.150157e-06\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 392.70013566191255\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 386.07550144678225\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 384.7477132506769\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 383.79536033230926\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 383.5027562287396\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 383.30440589529974\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 383.2045159612531\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 383.11796749955164\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 383.0662146995755\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 9, inertia 383.00393679076706\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 10, inertia 382.8498047900149\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 11, inertia 382.60380667041454\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 12, inertia 382.53254869840737\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 13, inertia 382.52808258782017\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 14, inertia 382.5243890573824\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 15, inertia 382.5243890573824\n",
            "center shift 0.000000e+00 within tolerance 4.150157e-06\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 369.3175190550365\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 354.5237270712984\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 351.18677674997014\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 350.4190677673848\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 350.1663911909235\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 350.0646107678915\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 350.0208034547549\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 350.0115747924538\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 350.0115747924538\n",
            "center shift 0.000000e+00 within tolerance 4.150157e-06\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 444.9134278145271\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 374.61583949608246\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 358.6717942381306\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 353.12893464010676\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 351.20454981404976\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 350.5924485899459\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 350.21297913643633\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 350.0961415933258\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 350.03628239429236\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 9, inertia 350.01782697665976\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 10, inertia 350.0123238487211\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 11, inertia 350.0123238487211\n",
            "center shift 0.000000e+00 within tolerance 4.150157e-06\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 442.4975740265198\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 420.91891480993297\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 416.46103494578347\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 413.01341924198266\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 406.3976581944694\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 400.95471839445094\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 399.28877466378276\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 398.7028680172928\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 398.5769902238766\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 9, inertia 398.54468778726465\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 10, inertia 398.52788702743857\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 11, inertia 398.51504874103006\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 12, inertia 398.475111575043\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 13, inertia 398.3928095626073\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 14, inertia 397.8118204429997\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 15, inertia 396.31665175135026\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 16, inertia 394.9539145242405\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 17, inertia 393.111873320906\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 18, inertia 387.3519380435153\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 19, inertia 372.9719728768812\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 20, inertia 371.25379214415483\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 21, inertia 371.06567435644126\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 22, inertia 371.03463188339265\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 23, inertia 371.03463188339265\n",
            "center shift 0.000000e+00 within tolerance 4.150157e-06\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 415.92610734892696\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 407.16707028875976\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 406.76116417223005\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 406.7258614073462\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 406.7084180269097\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 406.7005622619488\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 406.7005622619488\n",
            "center shift 0.000000e+00 within tolerance 4.150157e-06\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 401.5291485408809\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 367.7031618689201\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 353.8580523070713\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 350.43411764318074\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 350.09317026183635\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 350.03011143286966\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 350.01207974106956\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 350.01207974106956\n",
            "center shift 0.000000e+00 within tolerance 4.150157e-06\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 409.00351394933915\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 395.15267920991084\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 385.2841844753245\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 380.87612719389193\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 377.73831380932563\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 375.99102849798203\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 375.04027149283854\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 374.3771164962082\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 373.9646634348489\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 9, inertia 373.694663122826\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 10, inertia 373.54997676211093\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 11, inertia 373.49393490250947\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 12, inertia 373.4662056477279\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 13, inertia 373.41187346067613\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 14, inertia 373.390303068845\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 15, inertia 373.3862637238973\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 16, inertia 373.3862637238973\n",
            "center shift 0.000000e+00 within tolerance 4.150157e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_t6z8M7JRdm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "39861b44-849f-4c64-fb0c-04e13eaa7436"
      },
      "source": [
        "print (clusters)"
      ],
      "execution_count": 313,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3 3 3 ... 6 6 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECn1jgsNJkq3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "fb01636b-bf3c-4e1a-f9d4-9e4a47f25e20"
      },
      "source": [
        "print (names.T)\n",
        "print (kmeans.cluster_centers_)"
      ],
      "execution_count": 314,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                        0                    1   ...               17        18\n",
            "names  REGION-CENTROID-COL  REGION-CENTROID-ROW  ...  SATURATION-MEAN  HUE-MEAN\n",
            "\n",
            "[1 rows x 19 columns]\n",
            "[[5.35098814e-01 1.50166667e-01 0.00000000e+00 2.77777769e-02\n",
            "  1.66666667e-03 3.02281387e-02 5.42887957e-04 2.67660451e-02\n",
            "  5.86661900e-04 8.23246433e-01 7.79716377e-01 8.94170356e-01\n",
            "  7.88760696e-01 2.70665440e-01 6.66372551e-01 2.89386481e-01\n",
            "  8.94170356e-01 2.11804171e-01 1.25065773e-01]\n",
            " [7.48273727e-01 5.32040663e-01 0.00000000e+00 3.91566253e-02\n",
            "  3.76506024e-02 1.13530170e-01 1.89223845e-02 1.07311223e-01\n",
            "  1.76269993e-02 2.98573078e-01 2.77520947e-01 3.50080735e-01\n",
            "  2.63837104e-01 5.93300449e-01 4.49241676e-01 3.11452658e-01\n",
            "  3.50162801e-01 3.03046926e-01 1.64359304e-01]\n",
            " [2.53602814e-01 4.59865407e-01 0.00000000e+00 2.63459328e-02\n",
            "  1.37457045e-02 3.73368364e-02 2.36990363e-03 2.79012170e-02\n",
            "  2.02174405e-03 2.59422230e-02 1.77752903e-02 4.22796815e-02\n",
            "  1.64643108e-02 7.69272199e-01 2.16203765e-01 5.08343293e-01\n",
            "  4.32513955e-02 8.02618370e-01 1.80506442e-01]\n",
            " [5.13993692e-01 8.08936588e-01 0.00000000e+00 7.74410751e-02\n",
            "  5.05050505e-03 5.44737633e-02 1.40719343e-03 4.63349822e-02\n",
            "  1.40097198e-03 1.08789943e-01 9.14029557e-02 9.24140773e-02\n",
            "  1.42676436e-01 6.79161019e-01 7.90017879e-02 8.21286885e-01\n",
            "  1.34900800e-01 4.14491323e-01 8.92332630e-01]\n",
            " [2.51678367e-01 3.92748756e-01 0.00000000e+00 7.56218884e-02\n",
            "  1.94029851e-02 7.76573443e-02 4.14943092e-03 6.12403776e-02\n",
            "  5.03684355e-03 1.47427853e-01 1.37485185e-01 1.84391005e-01\n",
            "  1.17637066e-01 7.18250172e-01 3.43789293e-01 3.54453556e-01\n",
            "  1.84648188e-01 4.13413877e-01 2.02752484e-01]\n",
            " [3.02505527e-01 5.30861582e-01 0.00000000e+00 5.22598854e-02\n",
            "  4.66101695e-02 1.00816850e-01 9.42021613e-03 8.39719910e-02\n",
            "  1.10432753e-02 4.00608091e-01 3.70347226e-01 4.72460748e-01\n",
            "  3.53035779e-01 4.97146165e-01 5.70882355e-01 2.13054402e-01\n",
            "  4.72460748e-01 3.02263027e-01 1.63879166e-01]\n",
            " [7.69062512e-01 4.25930421e-01 0.00000000e+00 1.40237320e-02\n",
            "  2.26537217e-02 3.97024636e-02 2.98260626e-03 2.31160412e-02\n",
            "  2.09423436e-03 4.03849749e-02 3.44264190e-02 5.73849797e-02\n",
            "  2.80592355e-02 7.79917165e-01 2.22794598e-01 4.86886157e-01\n",
            "  5.83620495e-02 5.39152125e-01 2.44988039e-01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CflwOHZ1J9rQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "a273f81a-4e4e-4d55-bdb8-917d76bbebe9"
      },
      "source": [
        "import pylab as pl\n",
        "\n",
        "#pl.plot(x[clusters==0,0],x[clusters==0,2],'ro')\n",
        "#pl.plot(x[clusters==1,0],x[clusters==1,2],'bo')\n",
        "#pl.plot(x[clusters==2,0],x[clusters==2,2],'go')\n",
        "#pl.plot(x[clusters==3,0],x[clusters==3,2],'mo')\n",
        "#pl.plot(x[clusters==4,0],x[clusters==4,2],'co')\n",
        "#pl.plot(x[clusters==5,0],x[clusters==5,2],'yo')\n",
        "#pl.plot(x[clusters==6,0],x[clusters==6,2],'ko')\n",
        "#pl.show() #didn't work\n",
        "#other course method.\n",
        "print(\"{:>34} {} {} {} {} {} {}\".format(\"cluster0\", \"cluster1\", \"cluster2\", \n",
        "                                        \"cluster3\", \"cluster4\", \"cluster5\", \n",
        "                                        \"cluster6\"))\n",
        "\n",
        "for i in range(len(names.names)):\n",
        "    print('{:>25} {:f} {:f} {:f} {:f} {:f} {:f} {:f}'.format(\n",
        "        names.names[i], \n",
        "        kmeans.cluster_centers_[0][i], \n",
        "        kmeans.cluster_centers_[1][i], \n",
        "        kmeans.cluster_centers_[2][i],\n",
        "        kmeans.cluster_centers_[3][i],\n",
        "        kmeans.cluster_centers_[4][i],\n",
        "        kmeans.cluster_centers_[5][i],\n",
        "        kmeans.cluster_centers_[6][i]))\n",
        "  "
      ],
      "execution_count": 318,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                          cluster0 cluster1 cluster2 cluster3 cluster4 cluster5 cluster6\n",
            "      REGION-CENTROID-COL 0.535099 0.748274 0.253603 0.513994 0.251678 0.302506 0.769063\n",
            "      REGION-CENTROID-ROW 0.150167 0.532041 0.459865 0.808937 0.392749 0.530862 0.425930\n",
            "       REGION-PIXEL-COUNT 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000\n",
            "     SHORT-LINE-DENSITY-5 0.027778 0.039157 0.026346 0.077441 0.075622 0.052260 0.014024\n",
            "     SHORT-LINE-DENSITY-2 0.001667 0.037651 0.013746 0.005051 0.019403 0.046610 0.022654\n",
            "               VEDGE-MEAN 0.030228 0.113530 0.037337 0.054474 0.077657 0.100817 0.039702\n",
            "                 VEDGE-SD 0.000543 0.018922 0.002370 0.001407 0.004149 0.009420 0.002983\n",
            "               HEDGE-MEAN 0.026766 0.107311 0.027901 0.046335 0.061240 0.083972 0.023116\n",
            "                 HEDGE-SD 0.000587 0.017627 0.002022 0.001401 0.005037 0.011043 0.002094\n",
            "           INTENSITY-MEAN 0.823246 0.298573 0.025942 0.108790 0.147428 0.400608 0.040385\n",
            "              RAWRED-MEAN 0.779716 0.277521 0.017775 0.091403 0.137485 0.370347 0.034426\n",
            "             RAWBLUE-MEAN 0.894170 0.350081 0.042280 0.092414 0.184391 0.472461 0.057385\n",
            "            RAWGREEN-MEAN 0.788761 0.263837 0.016464 0.142676 0.117637 0.353036 0.028059\n",
            "               EXRED-MEAN 0.270665 0.593300 0.769272 0.679161 0.718250 0.497146 0.779917\n",
            "              EXBLUE-MEAN 0.666373 0.449242 0.216204 0.079002 0.343789 0.570882 0.222795\n",
            "             EXGREEN-MEAN 0.289386 0.311453 0.508343 0.821287 0.354454 0.213054 0.486886\n",
            "               VALUE-MEAN 0.894170 0.350163 0.043251 0.134901 0.184648 0.472461 0.058362\n",
            "          SATURATION-MEAN 0.211804 0.303047 0.802618 0.414491 0.413414 0.302263 0.539152\n",
            "                 HUE-MEAN 0.125066 0.164359 0.180506 0.892333 0.202752 0.163879 0.244988\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTrX3-6KM5lH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "befcae72-7299-47ba-8915-8923d09a1d0b"
      },
      "source": [
        "#Compare your 7 clusters to the 7 pre-assigned classes by \n",
        "#computing the Completeness and Homogeneity values of the generated clusters\n",
        "from sklearn.metrics import completeness_score, homogeneity_score\n",
        "print ('completeness_score = ',completeness_score(numeric_class_labels[1],clusters))\n",
        "print ('homogeneity_score = ',homogeneity_score(numeric_class_labels[1],clusters))"
      ],
      "execution_count": 316,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "completeness_score =  0.6117374684331666\n",
            "homogeneity_score =  0.6100499914689615\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfbqVivYXD5Y",
        "colab_type": "text"
      },
      "source": [
        "## c. \n",
        "\n",
        "Perform PCA on the normalized image data matrix. You may use the linear algebra package in Numpy or the Decomposition module in scikit-learn (the latter is much more efficient). Analyze the principal components to determine the number, r, of PCs needed to capture at least 95% of variance in the data. Then use these r components as features to transform the data into a reduced dimension space. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOyojs1VZ1oh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "0b1bb072-d4f3-4483-b07e-abd0239e61a8"
      },
      "source": [
        "from sklearn import decomposition\n",
        "pca = decomposition.PCA(n_components=7)\n",
        "pca_fit = pca.fit_transform(x)\n",
        "print('pca_fit = ',pca_fit, '\\n')\n",
        "print('pca.explained_variance_ratio_ = ', pca.explained_variance_ratio_, '\\n')\n",
        "print('pca.explained_variance_ratio_.sum() = ', pca.explained_variance_ratio_.sum())"
      ],
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pca_fit =  [[-0.68908218  0.53295103  0.24609833 ... -0.07643294  0.0478044\n",
            "  -0.04732089]\n",
            " [-0.66691952  0.51067465  0.33797172 ... -0.04117773  0.05655091\n",
            "  -0.04170747]\n",
            " [-0.71202748  0.77094365 -0.15582189 ... -0.16662228  0.04381397\n",
            "  -0.06069494]\n",
            " ...\n",
            " [-0.50774475 -0.12941533 -0.08224551 ... -0.02791769 -0.03189182\n",
            "  -0.10525024]\n",
            " [-0.47909057 -0.08634467 -0.15871141 ...  0.00443463 -0.00371285\n",
            "  -0.08505898]\n",
            " [-0.44225382 -0.10622525 -0.04712725 ...  0.01522668 -0.2061659\n",
            "   0.15294393]] \n",
            "\n",
            "pca.explained_variance_ratio_ =  [0.60714234 0.13196979 0.10123773 0.04543539 0.03547361 0.01988035\n",
            " 0.0189197 ] \n",
            "\n",
            "pca.explained_variance_ratio_.sum() =  0.9600589227704951\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVmndffIRUkg",
        "colab_type": "text"
      },
      "source": [
        "*  So these 7 principal components can capture at least 95% of the variance in the data (96% actually).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfMyJkuIXD90",
        "colab_type": "text"
      },
      "source": [
        "## d. \n",
        "\n",
        "Perform Kmeans again, but this time on the lower dimensional transformed data. Then, compute the Completeness and Homogeneity values of the new clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUYL7dRIZ1IL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f14658c1-5b31-4cb3-bd68-ca5a65281224"
      },
      "source": [
        "kmeans.fit(pca_fit)\n",
        "lowdim_kmeans = kmeans.fit_predict(pca_fit)"
      ],
      "execution_count": 298,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 337.5542783914823\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 310.33027814036393\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 293.56087989458337\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 288.1489932448002\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 286.8637914818663\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 286.51456237972735\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 286.4507270334449\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 286.44256836990616\n",
            "center shift 2.902417e-03 within tolerance 1.081479e-05\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 348.93671113495816\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 316.81078009706187\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 312.80779895004116\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 311.5232898101244\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 311.09342966058483\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 310.61255683810066\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 310.290571360849\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 310.1690939785976\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 310.12751341548244\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 9, inertia 310.09226202680327\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 10, inertia 310.05279578032264\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 11, inertia 309.97054410930883\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 12, inertia 309.9079076738611\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 13, inertia 309.8982742634596\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 14, inertia 309.8950211062783\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 15, inertia 309.89057826932964\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 16, inertia 309.88943851200037\n",
            "center shift 3.041538e-03 within tolerance 1.081479e-05\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 306.43284028587357\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 295.5705069361833\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 292.9107179310487\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 291.3844284712817\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 290.56197055052775\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 289.30575509278424\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 288.1035141530122\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 287.81895426960864\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 287.5803657687481\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 9, inertia 287.196573775778\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 10, inertia 286.7277948756792\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 11, inertia 286.48577089345747\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 12, inertia 286.44202961081805\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 13, inertia 286.44202961081805\n",
            "center shift 0.000000e+00 within tolerance 1.081479e-05\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 356.12383262754037\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 338.78833356139955\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 333.512013280914\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 330.14433135732054\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 327.1563513231983\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 322.0404265500042\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 315.47614390103865\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 312.4764467956644\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 312.05237527191053\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 9, inertia 311.97041849127413\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 10, inertia 311.95718024872417\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 11, inertia 311.95718024872417\n",
            "center shift 0.000000e+00 within tolerance 1.081479e-05\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 295.8498954205328\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 288.588630461492\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 287.8954372039393\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 287.51399212809815\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 286.93081282473537\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 286.53609033331827\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 286.45236456682994\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 286.44256836990616\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 286.44256836990616\n",
            "center shift 0.000000e+00 within tolerance 1.081479e-05\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 335.73668946409714\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 299.2684341917984\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 289.363221149011\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 287.05252889044664\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 286.61191878194455\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 286.5090977339203\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 286.4819129374697\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 286.4611083865646\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 286.4490355523968\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 9, inertia 286.4442670547256\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 10, inertia 286.4430670371849\n",
            "center shift 2.683138e-03 within tolerance 1.081479e-05\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 332.94500322671433\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 313.3851314663794\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 306.7354512845415\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 304.80248567888134\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 304.6145703210669\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 304.58229896779386\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 304.5620675949965\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 304.55687695023\n",
            "center shift 3.238233e-03 within tolerance 1.081479e-05\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 346.23277933743674\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 333.3668092618986\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 321.9125543250062\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 308.5143621490745\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 298.8254691365656\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 292.9125140493992\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 290.19088854093303\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 288.6935010557463\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 287.56725723585384\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 9, inertia 287.1306768344356\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 10, inertia 286.6594135797808\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 11, inertia 286.4688399444502\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 12, inertia 286.44202961081805\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 13, inertia 286.44202961081805\n",
            "center shift 0.000000e+00 within tolerance 1.081479e-05\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 317.2998622197324\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 293.51665989317326\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 288.4147916789555\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 287.14131482027597\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 286.69240254515006\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 286.47548206588647\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 286.4595249268409\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 286.4540171696502\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 286.4497168272991\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 9, inertia 286.4434566998334\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 10, inertia 286.44225668229274\n",
            "center shift 2.683138e-03 within tolerance 1.081479e-05\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 359.24733688325864\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 328.57647360727105\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 319.5516543431187\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 317.42850226490145\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 313.7528757350394\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 311.38587419479836\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 310.531598758011\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 310.19800550009825\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 310.0695768970303\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 9, inertia 310.0308524836409\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 10, inertia 310.01873320059264\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 11, inertia 310.00681206215916\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 12, inertia 309.9938310338191\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 13, inertia 309.984707731804\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 14, inertia 309.98208815889507\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 15, inertia 309.9808198532038\n",
            "center shift 1.872801e-03 within tolerance 1.081479e-05\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 323.9337547197963\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 304.4086483853816\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 296.9662965553242\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 291.3935092921452\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 288.3772522758907\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 287.00224610505006\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 286.73701288022795\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 286.6387978834452\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 286.50243872201315\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 9, inertia 286.45220324006493\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 10, inertia 286.441758015014\n",
            "center shift 2.794258e-03 within tolerance 1.081479e-05\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 321.31506236858945\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 306.3168294891674\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 303.0687779862364\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 302.0759062524789\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 301.8047905181807\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 301.4942736174791\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 301.24175564745155\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 301.14060655174154\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 301.09590094656915\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 9, inertia 301.084872338925\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 10, inertia 301.0621620358677\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 11, inertia 301.0351478123591\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 12, inertia 301.02874515211954\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 13, inertia 301.0151651059447\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 14, inertia 301.0151651059447\n",
            "center shift 0.000000e+00 within tolerance 1.081479e-05\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 315.61227336484933\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 308.4880037363969\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 307.04891779998\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 306.3967796285667\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 306.1486155014494\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 306.035315369327\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 305.9846465042416\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 305.95765572461704\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 305.9319081526439\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 9, inertia 305.91403944805944\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 10, inertia 305.8558263841219\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 11, inertia 305.81034557758795\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 12, inertia 305.77951176825593\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 13, inertia 305.7681354183171\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 14, inertia 305.7599272926233\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 15, inertia 305.7561171024339\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 16, inertia 305.7147581772746\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 17, inertia 305.6730881312271\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 18, inertia 305.5077909137276\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 19, inertia 305.19832706132735\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 20, inertia 304.78888361880445\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 21, inertia 304.20013492402325\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 22, inertia 303.8945926884883\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 23, inertia 303.72300274206367\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 24, inertia 303.62714435256987\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 25, inertia 303.60771334872857\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 26, inertia 303.58337260439214\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 27, inertia 303.5554552447596\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 28, inertia 303.5526043042115\n",
            "center shift 2.474658e-03 within tolerance 1.081479e-05\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 338.39368622746724\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 309.252832160396\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 306.5196164775805\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 304.68379705379124\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 301.9909968845898\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 297.31090789625034\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 290.73167429334967\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 287.8251791250345\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 286.94397912576073\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 9, inertia 286.55794716423094\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 10, inertia 286.482811015281\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 11, inertia 286.45761217054417\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 12, inertia 286.452307746183\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 13, inertia 286.447742894077\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 14, inertia 286.4434566998334\n",
            "center shift 2.907984e-03 within tolerance 1.081479e-05\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 331.2977202321586\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 309.5109296193483\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 306.45029710329914\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 305.2912032555935\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 305.01965941673456\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 304.8242866134437\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 304.66057670818986\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 304.59216516387596\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 304.56401755204126\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 9, inertia 304.55662973172065\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 10, inertia 304.55662973172065\n",
            "center shift 0.000000e+00 within tolerance 1.081479e-05\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 323.97141969172964\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 311.2164032511972\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 302.9678492734039\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 296.4810292833952\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 293.7908614094296\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 292.0319859821898\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 291.04782448035917\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 290.3975642407556\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 289.32620002246955\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 9, inertia 288.15538803013686\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 10, inertia 287.8295987193208\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 11, inertia 287.5762188481456\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 12, inertia 287.1411668479433\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 13, inertia 286.7133185311998\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 14, inertia 286.4899636681157\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 15, inertia 286.4464987229046\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 16, inertia 286.44256836990616\n",
            "center shift 2.660173e-03 within tolerance 1.081479e-05\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 320.4253408833753\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 301.12047546730076\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 292.5429582612099\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 288.6390629735602\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 287.2853892209471\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 286.79986415913606\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 286.53847999131364\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 286.4783834454057\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 286.4611083865646\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 9, inertia 286.4490355523968\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 10, inertia 286.4442670547256\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 11, inertia 286.4430670371849\n",
            "center shift 2.683138e-03 within tolerance 1.081479e-05\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 346.25616645165445\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 323.53274952131574\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 316.6239286966494\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 314.6825102080788\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 313.8744873566419\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 313.49106114180523\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 313.33192277794706\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 313.2050966173414\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 313.1369165352472\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 9, inertia 313.03362286670256\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 10, inertia 312.823472617472\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 11, inertia 312.494298441704\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 12, inertia 312.1026852990158\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 13, inertia 311.4745763139829\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 14, inertia 310.45024981184775\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 15, inertia 308.2726389179073\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 16, inertia 306.29375523652936\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 17, inertia 305.37872665989494\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 18, inertia 305.02825100976634\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 19, inertia 304.8997356977887\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 20, inertia 304.7066119522725\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 21, inertia 304.6532779424759\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 22, inertia 304.58304656111267\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 23, inertia 304.55982884254706\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 24, inertia 304.55687695023\n",
            "center shift 1.856695e-03 within tolerance 1.081479e-05\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 325.52489625014726\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 314.66739030116696\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 313.6879705780221\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 313.21558200621075\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 312.9313438601607\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 312.68746599014116\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 312.28897188632675\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 311.69532336384407\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 310.6396664802256\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 9, inertia 308.4567691443606\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 10, inertia 306.93651452907426\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 11, inertia 305.45923345912894\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 12, inertia 304.3111701788232\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 13, inertia 303.87063204200047\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 14, inertia 303.70180991115785\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 15, inertia 303.6276666990335\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 16, inertia 303.5924983482454\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 17, inertia 303.55517471045243\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 18, inertia 303.5423996419411\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 19, inertia 303.5423996419411\n",
            "center shift 0.000000e+00 within tolerance 1.081479e-05\n",
            "Initialization complete\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 0, inertia 320.14812262215037\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 1, inertia 304.30203214507867\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 2, inertia 302.1448449421003\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 3, inertia 301.52620357373627\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 4, inertia 301.21118491333345\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 5, inertia 301.1157350882353\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 6, inertia 301.0951630728365\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 7, inertia 301.0841344651923\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 8, inertia 301.06129609707114\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 9, inertia 301.0353566084067\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 10, inertia 301.02851033187085\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 11, inertia 301.01508305706227\n",
            "start iteration\n",
            "done sorting\n",
            "end inner loop\n",
            "Iteration 12, inertia 301.01508305706227\n",
            "center shift 0.000000e+00 within tolerance 1.081479e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2nssrp7R31t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "2db6c789-9caf-40c1-dac4-9db6568837ab"
      },
      "source": [
        "print('Without PCA:')\n",
        "print('completeness_score = ',completeness_score(numeric_class_labels[1],clusters))\n",
        "print('homogeneity_score = ',homogeneity_score(numeric_class_labels[1],clusters))\n",
        "\n",
        "print('With PCA:')\n",
        "print('completeness_score = ',completeness_score(numeric_class_labels[1],lowdim_kmeans))\n",
        "print('homogeneity_score = ',homogeneity_score(numeric_class_labels[1],lowdim_kmeans))\n"
      ],
      "execution_count": 299,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Without PCA:\n",
            "completeness_score =  0.6131870124853009\n",
            "homogeneity_score =  0.6115021163370862\n",
            "With PCA:\n",
            "completeness_score =  0.6107955063694607\n",
            "homogeneity_score =  0.6091364049733291\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jqY4QupXECN",
        "colab_type": "text"
      },
      "source": [
        "## e. \n",
        "\n",
        "Discuss your observations based on the comparison of the two clustering results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1CzUPEtZ0nA",
        "colab_type": "text"
      },
      "source": [
        "*   The completeness and homegeneity scores are very close'Without PCA' and 'With PCA'. This means that we can confidently use PCA to reduce dimensionality without losing explained variance or the underlying structures of this dataset.\n",
        "*  PCA with 7 components is recommended for this dataset. \n",
        "*  This is particularly important for image data as it can be quite cumbersome and memory intensive. Being able to reduce the data is an important step towards optimizing models and improving efficiency.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptoN7Ni8Tp3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}